{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import elasticsearch\n",
    "import string\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "from elasticsearch import Elasticsearch\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_TRAIN_FILEPATH = 'datasets\\DBpedia\\smarttask_dbpedia_train.json'\n",
    "QUERY_TEST_FILEPATH = 'datasets\\DBpedia\\smarttask_dbpedia_test_questions.json'\n",
    "INDEX_NAME = 'smart'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elasticsearch Default Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'will', 'as', 'was', 'if', 'a', 'at', 'an', 'is', 'these', 'are', 'there', 'but', 'in', 'into', 'it', 'this', 'or', 'their', 'by', 'that', 'and', 'the', 'then', 'such', 'of', 'not', 'to', 'with', 'no', 'they', 'for', 'be', 'on'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(['a', 'an', 'and', 'are', 'as', 'at', 'be', 'but', 'by', 'for', 'if', 'in', 'into', 'is', 'it', 'no', 'not', 'of', 'on', 'or', 'such', 'that', 'the', 'their', 'then', 'there', 'these', 'they', 'this', 'to', 'was', 'will', 'with'])\n",
    "print(stop_words) # Default in ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"Preprocess some query, lower, remove punctuation stuff, stopwords, etc.\"\"\"\n",
    "    text = text.strip().lower()\n",
    "    text = text.replace('_', ' ').replace('-', ' ')\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # Remove punctuation stuff.\n",
    "    text = re.sub('\\s\\s+', ' ', text).split(' ') # Replace consequtive whitespace with a single space.\n",
    "    return ' '.join([v for v in text if not v in stop_words]).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec - Convert GloVe to Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = datapath(os.getcwd()+'/datasets/gensim/gensim.6B.100d.txt')\n",
    "def convertGloveToGensim(target, output):\n",
    "    _ = glove2word2vec(datapath(os.getcwd()+target), datapath(os.getcwd()+output))\n",
    "#convertGloveToGensim('/datasets/glove/glove.6B.100d.txt', '/datasets/gensim/gensim.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'BERNTA-PC',\n",
       " 'cluster_name': 'elasticsearch',\n",
       " 'cluster_uuid': 'IP06yo9vScKZA1ZTb8R9HA',\n",
       " 'version': {'number': '7.9.2',\n",
       "  'build_flavor': 'default',\n",
       "  'build_type': 'zip',\n",
       "  'build_hash': 'd34da0ea4a966c4e49417f2da2f244e3e97b4e6e',\n",
       "  'build_date': '2020-09-23T00:45:33.626720Z',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '8.6.2',\n",
       "  'minimum_wire_compatibility_version': '6.8.0',\n",
       "  'minimum_index_compatibility_version': '6.0.0-beta1'},\n",
       " 'tagline': 'You Know, for Search'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = Elasticsearch()\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs: 4926217\n"
     ]
    }
   ],
   "source": [
    "es.indices.refresh(INDEX_NAME)\n",
    "count = es.cat.count(INDEX_NAME, params={\"format\": \"json\"})\n",
    "print('Docs:', int(count[0]['count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DOCS = int(count[0]['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_queries(filepath):\n",
    "    \"\"\"\n",
    "    Load training queries from a file. \n",
    "    Returns a dictoinary with queryID as key and corresponding query, category and type.\n",
    "    \"\"\"\n",
    "    query_dicts = {}\n",
    "    queries = None\n",
    "    with open(filepath, \"r\") as f:\n",
    "        queries = f.read()\n",
    "    \n",
    "    for query in json.loads(queries):\n",
    "        try:\n",
    "            qID, qText, qCat, qType = query[\"id\"].lower(), query[\"question\"].lower(), query[\"category\"].lower(), ' '.join(query[\"type\"]).lower()\n",
    "            if not 'dbo:' in qType: # Skip queries without a dbo: type.\n",
    "                continue\n",
    "            query_dicts[qID] = {\"query\": preprocess(qText), \"category\": qCat, \"type\": qType.replace('dbo:', '')}\n",
    "        except Exception as e:\n",
    "            # print(\"Query: {}\\n\\tThrew an exception: {}\\n\".format(query, e))\n",
    "            continue\n",
    "    return query_dicts\n",
    "\n",
    "def load_test_queries(filepath):\n",
    "    \"\"\"\n",
    "    Load test queries from a file.\n",
    "    Returns a dictionary with queryID as key, and corresponding query as a string.\n",
    "    \"\"\"\n",
    "    query_dicts = {}\n",
    "    queries = None\n",
    "    with open(filepath, \"r\") as f:\n",
    "        queries = f.read()\n",
    "    \n",
    "    for query in json.loads(queries):\n",
    "        try:\n",
    "            query_dicts[query[\"id\"].lower()] = {\"query\": preprocess(query[\"question\"].lower())}\n",
    "        except Exception as e:\n",
    "            # print(\"Query: {}\\n\\tThrew an exception: {}\\n\".format(query, e))\n",
    "            continue\n",
    "    return query_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training queries: 9557 \n",
      "\tExample key 'dbpedia_17655' returns: {'query': 'what town birthplace joseph greenberg', 'category': 'resource', 'type': 'city settlement populatedplace place location'}\n",
      "# test queries: 4369 \n",
      "\tExample key 'dbpedia_21099' contain: {'query': 'under which president did some politicians live kensington'}\n"
     ]
    }
   ],
   "source": [
    "training_queries = load_train_queries(QUERY_TRAIN_FILEPATH)\n",
    "test_queries = load_test_queries(QUERY_TEST_FILEPATH)\n",
    "\n",
    "print(\"# training queries:\", len(training_queries), \"\\n\\tExample key 'dbpedia_17655' returns:\", training_queries['dbpedia_17655'])\n",
    "print(\"# test queries:\", len(test_queries), \"\\n\\tExample key 'dbpedia_21099' contain:\", test_queries['dbpedia_21099'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache analyze query\n",
    "For each train / test query -> cache the respective analyze query terms.\n",
    "This will speed up the evaluation later.\n",
    "#### This will take 2-8 minutes! (depends on elasticsearch caching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_query(es, query, index=INDEX_NAME):\n",
    "    \"\"\"Analyzes a query with respect to the relevant index. \n",
    "    \n",
    "    Arguments:\n",
    "        es: Elasticsearch object instance.\n",
    "        query: String of query terms.\n",
    "        index: Name of the index with respect to which the query is analyzed.  \n",
    "    \n",
    "    Returns:\n",
    "        A list of query terms that exist in the abstract field among the documents in the index. \n",
    "    \"\"\"\n",
    "    tokens = es.indices.analyze(index=index, body={'text': query})['tokens']\n",
    "    query_terms = []\n",
    "    for t in sorted(tokens, key=lambda x: x['position']):\n",
    "        ## Use a boolean query to find at least one document that contains the term.\n",
    "        hits = es.search(index=index, body={'query': {'match': {'abstract': t['token']}}}, \n",
    "                                   _source=False, size=1).get('hits', {}).get('hits', {})\n",
    "        doc_id = hits[0]['_id'] if len(hits) > 0 else None\n",
    "        if doc_id is None:\n",
    "            continue\n",
    "        query_terms.append(t['token'])\n",
    "    return query_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed: 303.1173372268677\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "for qId, queryObject in training_queries.items():\n",
    "    training_queries[qId]['analyzed'] = analyze_query(es, queryObject['query'], INDEX_NAME)\n",
    "    \n",
    "for qId, queryObject in test_queries.items():\n",
    "    test_queries[qId]['analyzed'] = analyze_query(es, queryObject['query'], INDEX_NAME)\n",
    "    \n",
    "print(\"Time Elapsed:\", (time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'town', 'birthplace', 'joseph', 'greenberg']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_queries['dbpedia_17655']['analyzed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load evaluation types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDBPediaTypes():\n",
    "    kv = {}\n",
    "    max_depth = 0\n",
    "    with open('./evaluation/dbpedia/dbpedia_types.tsv', 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i == 0: # Skip header\n",
    "                continue\n",
    "            line = line.strip().lower().split('\\t')\n",
    "            if len(line) != 3:\n",
    "                continue\n",
    "            type_name, depth, parent_type = line[0].split(':')[-1], int(line[1]), line[-1].split(':')[-1]\n",
    "            if (len(type_name) == 0) or (len(parent_type) == 0):\n",
    "                continue\n",
    "            kv[type_name] = {'depth':depth, 'parent':parent_type}\n",
    "            max_depth = max(depth, max_depth)\n",
    "    return kv, max_depth\n",
    "\n",
    "def getTypeHierarchy(kv, items, target):\n",
    "    if not target in kv:\n",
    "        return\n",
    "    items.append(target)\n",
    "    getTypeHierarchy(kv, items, kv[target]['parent'])\n",
    "\n",
    "def buildDBPediaTypeHierarchy(kv, target, reverse=True):\n",
    "    items = [] # List of types, representing the hierarchy of the types related to the target.\n",
    "    getTypeHierarchy(kv, items, target)\n",
    "    if reverse:\n",
    "        return items[::-1] # Reverse the order to return the correct hierarchy where the first item = top level.\n",
    "    return items\n",
    "\n",
    "def cacheDBPediaPaths():\n",
    "    \"\"\"Simplify Evaluation Path Computations\"\"\"\n",
    "    for k in type_hierarchy.keys():\n",
    "        type_hierarchy[k]['path'] = buildDBPediaTypeHierarchy(type_hierarchy, k, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['basketballleague', 'naturalevent', 'province', 'lunarcrater'] Max Depth 7\n"
     ]
    }
   ],
   "source": [
    "type_hierarchy, max_depth = loadDBPediaTypes()\n",
    "print(list(type_hierarchy.keys())[:4], 'Max Depth', max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['work', 'writtenwork', 'comic']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buildDBPediaTypeHierarchy(type_hierarchy, 'comic') # Example hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed: 0.0010001659393310547\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "cacheDBPediaPaths()\n",
    "print(\"Time Elapsed:\", (time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Retrieval\n",
    "Implements Okapi BM25, uses the Elastic search inbuilt implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_baseline(es, amount=0, index=INDEX_NAME):\n",
    "    \"\"\"\n",
    "    Evaluate the BM25 baseline on our train queries.\n",
    "    \n",
    "    Arguments:\n",
    "        es: Elasticsearch object instance.\n",
    "        amount: How many queries to run, 0 = all.\n",
    "        index: Name of the index with respect to which the query is analyzed.  \n",
    "    \n",
    "    Returns:\n",
    "        A dictionary containing the queryIDs - list of retrieved instance types.\n",
    "    \"\"\"\n",
    "    progress, N = 0, len(training_queries)\n",
    "    results = {}\n",
    "    for qId, queryObject in training_queries.items():\n",
    "        query = queryObject['analyzed']\n",
    "        hits = es.search(index=index, _source=True, size=10,\n",
    "            body={\"query\": {\"bool\": {\"must\": {\"match\": {\"abstract\": ' '.join(query)}}, \"must_not\": {\"match\": {\"instance\": \"thing\"}}}}}\n",
    "        )['hits']['hits']\n",
    "        results[qId] = [obj['_source']['instance'] for obj in hits]\n",
    "\n",
    "        progress += 1\n",
    "        if (progress % 50) == 0:\n",
    "            print('Progress - {}/{} queries handled.'.format(progress, N))\n",
    "\n",
    "        if amount and (progress >= amount):\n",
    "            break\n",
    "\n",
    "    return results\n",
    "\n",
    "def tokens_to_vec(tokens, model):\n",
    "    \"\"\"\n",
    "    Convert a list of tokens to some word 2 vec representation which conforms to our model.\n",
    "    \n",
    "    Arguments:\n",
    "        tokens: A list of words.\n",
    "        model: A word2vec model.\n",
    "    \n",
    "    Returns:\n",
    "        A D-dim vector which represents the tokens - the embedding\n",
    "    \"\"\"\n",
    "    size = model.vectors.shape[1]\n",
    "    if len(tokens) == 0:\n",
    "        return np.zeros(size)        \n",
    "    embeddings = []\n",
    "    for v in tokens:\n",
    "        embeddings.append((model[v] if (v in model) else np.random.rand(size)))\n",
    "    return np.mean(embeddings, axis=0).reshape(1, -1) # Take the mean of our matrix and return it as a D-size vector.\n",
    "\n",
    "def evaluate_word2vec(es, model, k=1000, amount=0, index=INDEX_NAME):\n",
    "    \"\"\"\n",
    "    Evaluate ranking using word2vec method.\n",
    "    We are using pre-trained embeddings. Convert each query and related doc to word2vec format,\n",
    "    compare the similarity and re-rank the entries.\n",
    "    \n",
    "    Arguments:\n",
    "        es: Elasticsearch object instance.\n",
    "        model: word2vec model.\n",
    "        k: How many documents to handle per query.\n",
    "        amount: How many queries to run, 0 = all.\n",
    "        index: Name of the index with respect to which the query is analyzed.  \n",
    "    \n",
    "    Returns:\n",
    "        A dictionary containing the queryIDs - list of retrieved instance types.\n",
    "    \"\"\"\n",
    "    progress, N = 0, len(training_queries)\n",
    "    results = {}\n",
    "    for qId, queryObject in training_queries.items():\n",
    "        query = queryObject['analyzed']\n",
    "        hits = es.search(index=index, _source=True, size=k, \n",
    "            body={\"query\": {\"bool\": {\"must\": {\"match\": {\"abstract\": ' '.join(query)}}, \"must_not\": {\"match\": {\"instance\": \"thing\"}}}}}\n",
    "        )['hits']['hits']\n",
    "        queryEmbedding = tokens_to_vec(query, model)\n",
    "        rerank = []\n",
    "        for obj in hits:\n",
    "            docEmbedding = tokens_to_vec(obj['_source']['abstract'].split(), model)\n",
    "            sim = cosine_similarity(queryEmbedding, docEmbedding).item()\n",
    "            rerank.append((obj['_source']['instance'], sim))\n",
    "            \n",
    "        rerank.sort(key=lambda x:x[-1], reverse=True) # Re-rank the initial hits using our word2vec mdl.\n",
    "        results[qId] = [v for v,_ in rerank[:10]]\n",
    "\n",
    "        progress += 1\n",
    "        if (progress % 50) == 0:\n",
    "            print('Progress - {}/{} queries handled.'.format(progress, N))\n",
    "\n",
    "        if amount and (progress >= amount):\n",
    "            break\n",
    "\n",
    "    return results\n",
    "\n",
    "def evaluate_simple(es, k=1000, amount=0, index=INDEX_NAME):\n",
    "    \"\"\"\n",
    "    A test evaluation, simply re-rank using relevancy,\n",
    "    0 = Not relevant\n",
    "    1 = Partially relevant\n",
    "    2 = Relevant\n",
    "    \n",
    "    Arguments:\n",
    "        es: Elasticsearch object instance.\n",
    "        k: How many documents to handle per query.\n",
    "        amount: How many queries to run, 0 = all.\n",
    "        index: Name of the index with respect to which the query is analyzed.  \n",
    "    \n",
    "    Returns:\n",
    "        A dictionary containing the queryIDs - list of retrieved instance types.\n",
    "    \"\"\"\n",
    "    progress, N = 0, len(training_queries)\n",
    "    results = {}\n",
    "    for qId, queryObject in training_queries.items():\n",
    "        type_relevancy = {}\n",
    "        for typ in queryObject['type'].split(' '):\n",
    "            if not typ in type_hierarchy:\n",
    "                continue\n",
    "            hierarchy = buildDBPediaTypeHierarchy(type_hierarchy, typ)\n",
    "            for v in hierarchy:\n",
    "                type_relevancy[v] = 1 # Relevant, its in the same hierarchy but in a diff pos.            \n",
    "\n",
    "        for typ in queryObject['type'].split(' '):\n",
    "            type_relevancy[typ] = 2 # This is the type we want. Give it the highest weight.\n",
    "            \n",
    "        if len(type_relevancy) == 0:\n",
    "            continue\n",
    "\n",
    "        query = queryObject['analyzed']\n",
    "        hits = es.search(index=index, _source=True, size=k, \n",
    "            body={\"query\": {\"bool\": {\"must\": {\"match\": {\"abstract\": ' '.join(query)}}, \"must_not\": {\"match\": {\"instance\": \"thing\"}}}}}\n",
    "        )['hits']['hits']\n",
    "\n",
    "        rerank = []\n",
    "        for obj in hits:\n",
    "            instanceType = obj['_source']['instance']\n",
    "            if not instanceType in type_hierarchy:\n",
    "                rerank.append((instanceType, 0))\n",
    "                continue\n",
    "            if instanceType in type_relevancy:                \n",
    "                rerank.append((instanceType, type_relevancy[instanceType]))\n",
    "                continue                \n",
    "            weight = buildDBPediaTypeHierarchy(type_hierarchy, instanceType)\n",
    "            weight = [(1 if (t in type_relevancy) else 0) for t in weight] + [0]\n",
    "            rerank.append((instanceType, max(weight)))\n",
    "\n",
    "        rerank.sort(key=lambda x:x[-1], reverse=True) # Re-rank the initial hits based on their relevancy.\n",
    "        results[qId] = [v for v,_ in rerank[:10]]\n",
    "\n",
    "        progress += 1\n",
    "        if (progress % 50) == 0:\n",
    "            print('Progress - {}/{} queries handled.'.format(progress, N))\n",
    "\n",
    "        if amount and (progress >= amount):\n",
    "            break\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balog's Evaluation code, with some minor edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg(gains, k=5):\n",
    "    \"\"\"\n",
    "    Computes DCG for a given ranking.\n",
    "    Traditional DCG formula: DCG_k = sum_{i=1}^k gain_i / log_2(i+1).\n",
    "    \"\"\"\n",
    "    dcg = 0\n",
    "    for i in range(0, min(k, len(gains))):\n",
    "        dcg += gains[i] / math.log(i + 2, 2)\n",
    "    return dcg\n",
    "\n",
    "def ndcg(gains, ideal_gains, k=5):\n",
    "    \"\"\"Computes NDCG given gains for a ranking as well as the ideal gains.\"\"\"\n",
    "    try:\n",
    "        return dcg(gains, k) / dcg(ideal_gains, k)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_type_path(type, type_hierarchy):\n",
    "    \"\"\"\n",
    "    Gets the type's path in the hierarchy (excluding the root type, like owl:Thing).\n",
    "    The path for each type is computed only once then cached in type_hierarchy,\n",
    "    to save computation.\n",
    "    \"\"\"\n",
    "    if not type in type_hierarchy:\n",
    "        type_hierarchy[type] = {'depth':1, 'parent':'', 'path':[type]}\n",
    "    return type_hierarchy[type]['path']\n",
    "\n",
    "def get_type_distance(type1, type2, type_hierarchy):\n",
    "    \"\"\"\n",
    "    Computes the distance between two types in the hierarchy.\n",
    "    Distance is defined to be the number of steps between them in the hierarchy,\n",
    "    if they lie on the same path (which is 0 if the two types match), and\n",
    "    infinity otherwise.\n",
    "    \"\"\"\n",
    "    type1_path = get_type_path(type1, type_hierarchy)\n",
    "    type2_path = get_type_path(type2, type_hierarchy)\n",
    "    distance = math.inf\n",
    "    if type1 in type2_path:\n",
    "        distance = type2_path.index(type1)\n",
    "    if type2 in type1_path:\n",
    "        distance = min(type1_path.index(type2), distance)\n",
    "    return distance\n",
    "\n",
    "def get_most_specific_types(types, type_hierarchy):\n",
    "    \"\"\"Filters a set of input types to most specific types w.r.t the type\n",
    "    hierarchy; i.e., super-types are removed.\"\"\"\n",
    "    filtered_types = set(types)\n",
    "    for type in types:\n",
    "        type_path = get_type_path(type, type_hierarchy)\n",
    "        for supertype in type_path[1:]:\n",
    "            if supertype in filtered_types:\n",
    "                filtered_types.remove(supertype)\n",
    "    return filtered_types\n",
    "\n",
    "def get_expanded_types(types, type_hierarchy):\n",
    "    \"\"\"Expands a set of types with both more specific and more generic types\n",
    "    (i.e., all super-types and sub-types).\"\"\"\n",
    "    expanded_types = set()\n",
    "    for type in types:\n",
    "        # Adding all supertypes.\n",
    "        expanded_types.update(get_type_path(type, type_hierarchy))\n",
    "        # Adding all subtypes (NOTE: this bit could be done more efficiently).\n",
    "        for type2 in type_hierarchy:\n",
    "            if type_hierarchy[type2]['depth'] <= type_hierarchy[type]['depth']:\n",
    "                continue\n",
    "            type2_path = get_type_path(type2, type_hierarchy)\n",
    "            if type in type2_path:\n",
    "                expanded_types.update(type2_path)\n",
    "    return expanded_types\n",
    "\n",
    "def compute_type_gains(predicted_types, gold_types, type_hierarchy, max_depth):\n",
    "    \"\"\"Computes gains for a ranked list of type predictions.\n",
    "\n",
    "    Following the definition of Linear gain in (Balog and Neumayer, CIKM'12),\n",
    "    the gain for a given predicted type is 0 if it is not on the same path with\n",
    "    any of the gold types, and otherwise it's $1-d(t,t_q)/h$ where $d(t,t_q)$ is\n",
    "    the distance between the predicted type and the closest matching gold type\n",
    "    in the type hierarchy and h is the maximum depth of the type hierarchy.\n",
    "\n",
    "    Args:\n",
    "        predicted_types: Ranked list of predicted types.\n",
    "        gold_types: List/set of gold types (i.e., perfect answers).\n",
    "        type_hierarchy: Dict with type hierarchy.\n",
    "        max_depth: Maximum depth of the type hierarchy.\n",
    "\n",
    "    Returns:\n",
    "        List with gain values corresponding to each item in predicted_types.\n",
    "    \"\"\"\n",
    "    gains = []\n",
    "    expanded_gold_types = get_expanded_types(gold_types, type_hierarchy)\n",
    "    for predicted_type in predicted_types:\n",
    "        if predicted_type in expanded_gold_types:\n",
    "            # Since not all gold types may lie on the same branch, we take the\n",
    "            # closest gold type for determining distance.\n",
    "            min_distance = math.inf\n",
    "            for gold_type in gold_types:\n",
    "                min_distance = min(get_type_distance(predicted_type, gold_type,\n",
    "                                                     type_hierarchy),\n",
    "                                   min_distance)\n",
    "            gains.append(1 - min_distance / max_depth)\n",
    "        else:\n",
    "            gains.append(0)\n",
    "    return gains\n",
    "\n",
    "def evaluate(result):\n",
    "    \"\"\"\n",
    "    Evaluate the resulting dictionary, compute accuracy, strict and fuzzy ndcg_5, ndcg_10 where\n",
    "    ndcg_5 and ndcg_10 is computed using lenient NDCG@k with a Linear decay.\n",
    "    \n",
    "    Arguments:\n",
    "        result: A dictionary with queryIDs: List of retrieved types from the top 10 docs, \n",
    "        and a bool indicating if there was a perfect match.\n",
    "    \"\"\"\n",
    "    accuracy = []\n",
    "    strict_ndcg_5, strict_ndcg_10 = [], []\n",
    "    fuzzy_ndcg_5, fuzzy_ndcg_10 = [], []\n",
    "    for qId, obj in training_queries.items():\n",
    "        if qId not in result:\n",
    "            continue\n",
    "\n",
    "        qTypes = obj['type'].split(' ')\n",
    "        if len(qTypes) == 0:\n",
    "            continue\n",
    "\n",
    "        predicted_type = result[qId]\n",
    "        predicted_type_strict = [(1 if (t in obj['type']) else 0) for t in predicted_type]        \n",
    "        exact_match = max(predicted_type_strict + [0]) # Yes / No was there an explicit match?\n",
    "        \n",
    "        # Filters obj types to most specific ones in the hierarchy.\n",
    "        obj_types = get_most_specific_types(qTypes, type_hierarchy)\n",
    "        gains = compute_type_gains(predicted_type, obj_types, type_hierarchy, max_depth)\n",
    "        ideal_gains = sorted(gains, reverse=True)\n",
    "\n",
    "        accuracy.append(exact_match)\n",
    "        \n",
    "        strict_ndcg_5.append(ndcg(predicted_type_strict, sorted(predicted_type_strict, reverse=True), k=5))\n",
    "        strict_ndcg_10.append(ndcg(predicted_type_strict, sorted(predicted_type_strict, reverse=True), k=10))\n",
    "        \n",
    "        fuzzy_ndcg_5.append(ndcg(gains, ideal_gains, k=5))\n",
    "        fuzzy_ndcg_10.append(ndcg(gains, ideal_gains, k=10))\n",
    "        \n",
    "    print('Evaluation results (based on {} questions):'.format(len(accuracy)))\n",
    "    print('-------------------')\n",
    "    \n",
    "    print('Exact Type Prediction')\n",
    "    print('  Accuracy: {:5.3f}'.format(sum(accuracy) / len(accuracy)))\n",
    "    \n",
    "    print('Strict Type ranking')\n",
    "    print('  NDCG@5:  {:5.3f}'.format(sum(strict_ndcg_5) / len(strict_ndcg_5)))\n",
    "    print('  NDCG@10: {:5.3f}'.format(sum(strict_ndcg_10) / len(strict_ndcg_10)))\n",
    "    \n",
    "    print('Fuzzy Type ranking')\n",
    "    print('  NDCG@5:  {:5.3f}'.format(sum(fuzzy_ndcg_5) / len(fuzzy_ndcg_5)))\n",
    "    print('  NDCG@10: {:5.3f}'.format(sum(fuzzy_ndcg_10) / len(fuzzy_ndcg_10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing/Reading evaluation results to/from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_result_to_file(res, file):\n",
    "    with open('./results/{}.csv'.format(file), 'w') as f:\n",
    "        for qId, obj in res.items():\n",
    "            f.write('{},{}\\n'.format(qId, ' '.join(obj)))\n",
    "\n",
    "def read_result_from_file(file):\n",
    "    result = {}\n",
    "    with open('./results/{}.csv'.format(file), 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(',')\n",
    "            if len(line) != 2:\n",
    "                continue\n",
    "            result[line[0]] = [v for v in line[-1].split(' ') if len(v) > 0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "res_baseline = evaluate_baseline(es)\n",
    "print(\"Time Elapsed:\", (time.time()-start))\n",
    "write_result_to_file(res_baseline, 'baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results (based on 9557 questions):\n",
      "-------------------\n",
      "Exact Type Prediction\n",
      "  Accuracy: 0.492\n",
      "Strict Type ranking\n",
      "  NDCG@5:  0.237\n",
      "  NDCG@10: 0.323\n",
      "Fuzzy Type ranking\n",
      "  NDCG@5:  0.312\n",
      "  NDCG@10: 0.414\n"
     ]
    }
   ],
   "source": [
    "res_baseline = read_result_from_file('baseline')\n",
    "evaluate(res_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate advanced - Word2Vec ~ 45 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(EMBEDDING_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "res_word2vec = evaluate_word2vec(es, model, k=300)\n",
    "print(\"Time Elapsed:\", time.time()-start)\n",
    "write_result_to_file(res_word2vec, 'advanced_word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results (based on 9557 questions):\n",
      "-------------------\n",
      "Exact Type Prediction\n",
      "  Accuracy: 0.522\n",
      "Strict Type ranking\n",
      "  NDCG@5:  0.280\n",
      "  NDCG@10: 0.367\n",
      "Fuzzy Type ranking\n",
      "  NDCG@5:  0.364\n",
      "  NDCG@10: 0.455\n"
     ]
    }
   ],
   "source": [
    "res_word2vec = read_result_from_file('advanced_word2vec')\n",
    "evaluate(res_word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Method, Pointwise - Classifier\n",
    "Declare documents as relevant: \n",
    "* 0 - Not relevant\n",
    "* 1 - Kinda relevant\n",
    "* 2 - Totally relevant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOB_TERM_DOC_FREQ = {} # Save computations by storing term->docFrequency in a dict. Every query will run this K times so.. be clever!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(query_terms, doc_id, es, index=INDEX_NAME):\n",
    "    \"\"\"\n",
    "    Extracts query features, document features and query-document features of a query and document pair.\n",
    "    \n",
    "        Arguments:\n",
    "            query_terms: List of analyzed query terms.\n",
    "            doc_id: Document identifier of indexed document.\n",
    "            es: Elasticsearch object instance.\n",
    "            index: Name of relevant index on the running Elasticsearch service. \n",
    "            \n",
    "        Returns:\n",
    "            List of extracted feature values in a fixed order.\n",
    "    \"\"\"\n",
    "    query = dict(Counter(query_terms))    \n",
    "    doc_term_freqs = {} # Term frequencies in the document.\n",
    "    tv = es.termvectors(index=index, id=doc_id, fields='abstract', term_statistics=False)    \n",
    "    for term, term_stat in tv['term_vectors']['abstract']['terms'].items():\n",
    "        doc_term_freqs[term] = term_stat['term_freq']\n",
    "    \n",
    "    idf = []\n",
    "    for term in query_terms:\n",
    "        if not term in GLOB_TERM_DOC_FREQ:\n",
    "            n = 0\n",
    "            hits = es.search(\n",
    "                index=index, \n",
    "                body={\"query\": {\"bool\": {\"must\": {\"match\": {\"abstract\": term}}, \"must_not\": {\"match\": {\"instance\": \"thing\"}}}}}, \n",
    "                _source=False, size=1).get('hits',{}).get('hits',{})\n",
    "            doc_id = (hits[0]['_id'] if (len(hits) > 0) else None)\n",
    "            if doc_id is not None:\n",
    "                tv = es.termvectors(index=index, id=doc_id, fields='abstract', term_statistics=True)['term_vectors']['abstract']['terms']\n",
    "                if term in tv:\n",
    "                    n = tv[term]['doc_freq']                    \n",
    "            GLOB_TERM_DOC_FREQ[term] = n\n",
    "            \n",
    "        n = GLOB_TERM_DOC_FREQ[term]\n",
    "        if n: # Must be greater than > 0\n",
    "            idf.append(math.log(NUM_DOCS/n))\n",
    "\n",
    "    terms_doc_unique = [v for k,v in doc_term_freqs.items() if k in query] # Unique to query and doc.\n",
    "    \n",
    "    return [\n",
    "        len(query_terms),\n",
    "        sum(idf),\n",
    "        max([0] + idf),\n",
    "        (sum(idf) / max(len(idf),1)),\n",
    "        sum(doc_term_freqs.values()),\n",
    "        len(terms_doc_unique),\n",
    "        sum(terms_doc_unique),\n",
    "        max([0] + terms_doc_unique),\n",
    "        (sum(terms_doc_unique) / max(len(query.keys()),1))\n",
    "    ]\n",
    "\n",
    "def evaluate_l2r(es, k=200, amount=0, index=INDEX_NAME):\n",
    "    \"\"\"\n",
    "    Train a model, generate X - feature vectors and y - relevance labels.\n",
    "    \n",
    "    Relevancy is defined as such,\n",
    "    0 = Not relevant\n",
    "    1 = Partially relevant\n",
    "    2 = Relevant\n",
    "    \n",
    "    Arguments:\n",
    "        es: Elasticsearch object instance.\n",
    "        k: How many documents to handle per query.\n",
    "        amount: How many queries to run, 0 = all.\n",
    "        index: Name of the index with respect to which the query is analyzed.  \n",
    "    \n",
    "    Returns:\n",
    "        Returns a list of feature vectors, a list of their respective labels (relevance) and a list of \n",
    "        instances (type instances) noted from every retrieved doc.\n",
    "    \"\"\"\n",
    "    progress, N = 0, len(training_queries)\n",
    "    X, y, instances = [], [], []\n",
    "    for qId, queryObject in training_queries.items():\n",
    "        type_relevancy = {}        \n",
    "        for typ in queryObject['type'].split(' '):\n",
    "            if not typ in type_hierarchy:\n",
    "                continue\n",
    "            hierarchy = buildDBPediaTypeHierarchy(type_hierarchy, typ)\n",
    "            for v in hierarchy:\n",
    "                type_relevancy[v] = 1 # Relevant, its in the same hierarchy but in a diff pos.            \n",
    "                \n",
    "        for typ in queryObject['type'].split(' '):\n",
    "            type_relevancy[typ] = 2 # This is the type we want. Give it the highest weight.\n",
    "            \n",
    "        if len(type_relevancy) == 0:\n",
    "            continue\n",
    "\n",
    "        query = queryObject['analyzed']\n",
    "        hits = es.search(index=index, _source=True, size=k, \n",
    "            body={\"query\": {\"bool\": {\"must\": {\"match\": {\"abstract\": ' '.join(query)}}, \"must_not\": {\"match\": {\"instance\": \"thing\"}}}}}\n",
    "        )['hits']['hits']\n",
    "\n",
    "        for obj in hits:\n",
    "            relevancy = 0 # Default = not relevant\n",
    "            instanceType = obj['_source']['instance']\n",
    "            if instanceType in type_relevancy:\n",
    "                relevancy = type_relevancy[instanceType]\n",
    "            elif instanceType in type_hierarchy:                \n",
    "                relevancy = buildDBPediaTypeHierarchy(type_hierarchy, instanceType)\n",
    "                relevancy = max([(1 if (t in type_relevancy) else 0) for t in relevancy] + [0])\n",
    "            y.append(relevancy)\n",
    "            X.append(extract_features(query, obj['_id'], es, index))\n",
    "            instances.append(instanceType)\n",
    "\n",
    "        progress += 1\n",
    "        if (progress % 50) == 0:\n",
    "            print('Progress - {}/{} queries handled.'.format(progress, N))\n",
    "\n",
    "        if amount and (progress >= amount):\n",
    "            break\n",
    "\n",
    "    return X, y, instances\n",
    "\n",
    "def evaluate_l2r_rerank(es, model, X, instances, k=200, amount=0, index=INDEX_NAME):\n",
    "    \"\"\"\n",
    "    Evaluate our l2r model. Re-rank predicting relevancy. Sort by relevancy. High -> Low\n",
    "    \n",
    "    Relevancy is defined as such,\n",
    "    0 = Not relevant\n",
    "    1 = Partially relevant\n",
    "    2 = Relevant\n",
    "    \n",
    "    Arguments:\n",
    "        es: Elasticsearch object instance.\n",
    "        model: L2R classifier.\n",
    "        instances: A list of the type instances.\n",
    "        k: How many documents to handle per query.\n",
    "        amount: How many queries to run, 0 = all.\n",
    "        index: Name of the index with respect to which the query is analyzed.  \n",
    "    \n",
    "    Returns:\n",
    "        A dictionary containing the queryIDs - list of retrieved instance types.\n",
    "    \"\"\"\n",
    "    progress, N = 0, len(training_queries)\n",
    "    results, idx = {}, 0\n",
    "    predictions = (model.predict(X[:(k*amount)]) if amount else model.predict(X))\n",
    "    \n",
    "    for qId, queryObject in training_queries.items():\n",
    "        query = queryObject['analyzed']\n",
    "        hits = es.search(index=index, _source=False, size=k, \n",
    "            body={\"query\": {\"bool\": {\"must\": {\"match\": {\"abstract\": ' '.join(query)}}, \"must_not\": {\"match\": {\"instance\": \"thing\"}}}}}\n",
    "        )['hits']['hits']\n",
    "        rerank = []\n",
    "        for _ in hits:\n",
    "            rerank.append((instances[idx], predictions[idx]))\n",
    "            idx += 1\n",
    "        rerank.sort(key=lambda x:x[-1], reverse=True) # Re-rank the initial hits based on their relevancy.\n",
    "        results[qId] = [v for v,_ in rerank[:10]]\n",
    "\n",
    "        progress += 1\n",
    "        if (progress % 50) == 0:\n",
    "            print('Progress - {}/{} queries handled.'.format(progress, N))\n",
    "\n",
    "        if amount and (progress >= amount):\n",
    "            break\n",
    "\n",
    "    return results\n",
    "\n",
    "def evaluate_l2r_test(es, model, k=100, amount=0, index=INDEX_NAME):\n",
    "    \"\"\"\n",
    "    Predict the types for test queries. Re-rank the k docs and pick the majority vote out of the top 10.\n",
    "    If no majority vote, use the top doc type.\n",
    "    \n",
    "    Arguments:\n",
    "        es: Elasticsearch object instance.\n",
    "        model: L2R classifier.\n",
    "        k: How many documents to handle per query.\n",
    "        amount: How many queries to run, 0 = all.\n",
    "        index: Name of the index with respect to which the query is analyzed.  \n",
    "    \n",
    "    Returns:\n",
    "        A dictionary containing the queryIDs and its predicted type.\n",
    "    \"\"\"\n",
    "    progress, N = 0, len(test_queries)\n",
    "    results = {}\n",
    "    for qId, queryObject in test_queries.items():\n",
    "        query = queryObject['analyzed']\n",
    "        hits = es.search(index=index, _source=True, size=k, \n",
    "            body={\"query\": {\"bool\": {\"must\": {\"match\": {\"abstract\": ' '.join(query)}}, \"must_not\": {\"match\": {\"instance\": \"thing\"}}}}}\n",
    "        )['hits']['hits']\n",
    "        feat_vecs, instances = [], []\n",
    "        for obj in hits:\n",
    "            feat_vecs.append(extract_features(query, obj['_id'], es, index))\n",
    "            instances.append(obj['_source']['instance'])\n",
    "        if len(instances) == 0: # No hits!\n",
    "            results[qId] = 'N/A'\n",
    "            continue\n",
    "        instances_rerank = [instances[idx] for idx in np.argsort(model.predict(feat_vecs))[::-1]]\n",
    "        results[qId] = Counter(instances_rerank[:10]).most_common(1)[0][0]\n",
    "\n",
    "        progress += 1\n",
    "        if (progress % 50) == 0:\n",
    "            print('Progress - {}/{} queries handled.'.format(progress, N))\n",
    "\n",
    "        if amount and (progress >= amount):\n",
    "            break\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Pointwise Method (50+ min!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "X, y, instances = evaluate_l2r(es)\n",
    "print(\"Time Elapsed:\", time.time()-start)\n",
    "\n",
    "l2r_model = RandomForestClassifier(n_estimators = 100)\n",
    "_ = l2r_model.fit(X, y)\n",
    "\n",
    "start = time.time()\n",
    "res_advanced_pntwse = evaluate_l2r_rerank(es, l2r_model, X, instances)\n",
    "print(\"Time Elapsed:\", time.time()-start)\n",
    "\n",
    "write_result_to_file(res_advanced_pntwse, 'advanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results (based on 9557 questions):\n",
      "-------------------\n",
      "Exact Type Prediction\n",
      "  Accuracy: 0.776\n",
      "Strict Type ranking\n",
      "  NDCG@5:  0.731\n",
      "  NDCG@10: 0.754\n",
      "Fuzzy Type ranking\n",
      "  NDCG@5:  0.753\n",
      "  NDCG@10: 0.780\n"
     ]
    }
   ],
   "source": [
    "res_advanced_pntwse = read_result_from_file('advanced')\n",
    "evaluate(res_advanced_pntwse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc, type prediction using test queries. Predict the type of a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_type_res = evaluate_l2r_test(es, l2r_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_test_types(res, file):\n",
    "    with open('./results/{}.csv'.format(file), 'w') as f:\n",
    "        for qId, obj in res.items():\n",
    "            f.write('{},{}\\n'.format(qId, obj))\n",
    "            \n",
    "def read_test_types(file):\n",
    "    result = {}\n",
    "    with open('./results/{}.csv'.format(file), 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(',')\n",
    "            if len(line) != 2:\n",
    "                continue\n",
    "            result[line[0]] = line[1]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_test_types(test_type_res, 'test_type_predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: how many platforms does tomb raider have \n",
      "Type: videogame\n"
     ]
    }
   ],
   "source": [
    "test_type_res = read_test_types('test_type_predictions')\n",
    "print('Query:', test_queries['dbpedia_687']['query'], '\\nType:', test_type_res['dbpedia_687'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
