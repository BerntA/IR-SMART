{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import elasticsearch\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_TRAIN_FILEPATH = 'datasets\\DBpedia\\smarttask_dbpedia_train.json'\n",
    "QUERY_TEST_FILEPATH = 'datasets\\DBpedia\\smarttask_dbpedia_test_questions.json'\n",
    "INDEX_NAME = 'smart'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'their', 'these', 'into', 'such', 'this', 'then', 'they', 'to', 'of', 'with', 'at', 'not', 'if', 'there', 'a', 'by', 'it', 'that', 'the', 'as', 'but', 'on', 'and', 'are', 'no', 'or', 'was', 'be', 'is', 'for', 'will', 'in', 'an'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(['a', 'an', 'and', 'are', 'as', 'at', 'be', 'but', 'by', 'for', 'if', 'in', 'into', 'is', 'it', 'no', 'not', 'of', 'on', 'or', 'such', 'that', 'the', 'their', 'then', 'there', 'these', 'they', 'this', 'to', 'was', 'will', 'with'])\n",
    "print(stop_words) # Default in ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.strip().lower()\n",
    "    text = text.replace('_', ' ').replace('-', ' ')\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # Remove punctuation stuff.\n",
    "    text = re.sub('\\s\\s+', ' ', text).split(' ') # Replace consequtive whitespace with a single space.\n",
    "    return ' '.join([v for v in text if not v in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'name': 'BERNTA-PC',\n",
       " 'cluster_name': 'elasticsearch',\n",
       " 'cluster_uuid': 'IP06yo9vScKZA1ZTb8R9HA',\n",
       " 'version': {'number': '7.9.2',\n",
       "  'build_flavor': 'default',\n",
       "  'build_type': 'zip',\n",
       "  'build_hash': 'd34da0ea4a966c4e49417f2da2f244e3e97b4e6e',\n",
       "  'build_date': '2020-09-23T00:45:33.626720Z',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '8.6.2',\n",
       "  'minimum_wire_compatibility_version': '6.8.0',\n",
       "  'minimum_index_compatibility_version': '6.0.0-beta1'},\n",
       " 'tagline': 'You Know, for Search'}"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "es = Elasticsearch()\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Docs: 4926217\n"
     ]
    }
   ],
   "source": [
    "es.indices.refresh(INDEX_NAME)\n",
    "count = es.cat.count(INDEX_NAME, params={\"format\": \"json\"})\n",
    "print('Docs:', int(count[0]['count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_queries(filepath):\n",
    "    \"\"\"\n",
    "    Load training queries from a file. \n",
    "    Returns a dictoinary with queryID as key and corresponding query, category and type.\n",
    "    \"\"\"\n",
    "    query_dicts = {}\n",
    "    with open(filepath, \"r\") as f:\n",
    "      queries = f.read()\n",
    "    \n",
    "    for query in json.loads(queries):\n",
    "        try:\n",
    "            qID, qText, qCat, qType = query[\"id\"].lower(), query[\"question\"].lower(), query[\"category\"].lower(), ' '.join(query[\"type\"]).lower()\n",
    "            if not 'dbo:' in qType: # Skip queries without a dbo: type.\n",
    "                continue\n",
    "            query_dicts[qID] = {\"query\": preprocess(qText), \"category\": qCat, \"type\": qType}\n",
    "        except Exception as e:\n",
    "            # print(\"Query: {}\\n\\tThrew an exception: {}\\n\".format(query, e))\n",
    "            continue\n",
    "    return query_dicts\n",
    "\n",
    "def load_test_queries(filepath):\n",
    "    \"\"\"\n",
    "    Load test queries from a file.\n",
    "    Returns a dictionary with queryID as key, and corresponding query as a string.\n",
    "    \"\"\"\n",
    "    query_dicts = {}\n",
    "    with open(filepath, \"r\") as f:\n",
    "      queries = f.read()\n",
    "    \n",
    "    for query in json.loads(queries):\n",
    "        try:\n",
    "            query_dicts[query[\"id\"].lower()] = {\"query\": preprocess(query[\"question\"].lower())}\n",
    "        except Exception as e:\n",
    "            # print(\"Query: {}\\n\\tThrew an exception: {}\\n\".format(query, e))\n",
    "            continue\n",
    "    return query_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# training queries: 9557 \n\tExample key'dbpedia_17655' returns: {'query': 'what town birthplace joseph greenberg', 'category': 'resource', 'type': 'dbo:city dbo:settlement dbo:populatedplace dbo:place dbo:location'}\n# test queries: 4369 \n\tExample key'dbpedia_21099' contain: {'query': 'under which president did some politicians live kensington'}\n"
     ]
    }
   ],
   "source": [
    "training_queries = load_train_queries(QUERY_TRAIN_FILEPATH)\n",
    "test_queries = load_test_queries(QUERY_TEST_FILEPATH)\n",
    "\n",
    "print(\"# training queries:\", len(training_queries), \"\\n\\tExample key'dbpedia_17655' returns:\", training_queries['dbpedia_17655'])\n",
    "print(\"# test queries:\", len(test_queries), \"\\n\\tExample key'dbpedia_21099' contain:\", test_queries['dbpedia_21099'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Retrieval\n",
    "Implements Okapi BM25, uses the Elastic search inbuilt implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def internal_BM25(query, k = 100, field = 'abstract', index = INDEX_NAME):\n",
    "    \"\"\"\n",
    "    Perform baseline retrieval on a index using the inbuilt BM25 index\n",
    "\n",
    "    Arguments:\n",
    "        index: string\n",
    "        query: string, space separated terms\n",
    "        k: integer\n",
    "    \n",
    "    Returns:\n",
    "        List of k first entity IDs(string)\n",
    "    \"\"\"\n",
    "    hits = es.search(index=index, body={'query': {'match': {field: query}}}, _source=False, size=k).get('hits', {}).get('hits', {})\n",
    "    hits_ids = [obj['_id'] for obj in hits]\n",
    "    hits_types = [es.get(index=index, id=doc)[\"_source\"].get(\"instance\", \"thing\") for doc in hits_ids]\n",
    "    return Counter([obj for obj in hits_types if len(obj) > 0]).most_common()\n",
    "    \n",
    "def internal_BM25_score(query, k = 100, field = 'abstract', index = INDEX_NAME):\n",
    "    \"\"\"\n",
    "    Perform baseline retrieval on a index using the inbuilt BM25 index\n",
    "\n",
    "    Arguments:\n",
    "        index: string\n",
    "        query: string, space separated terms\n",
    "        k: integer\n",
    "    \n",
    "    Returns:\n",
    "        List of k first entity IDs(string), and corresponding score(double)\n",
    "    \"\"\"\n",
    "    hits = es.search(index=index, body={'query': {'match': {field: query}}}, _source=False, size=k).get('hits', {}).get('hits', {})\n",
    "    hits.sort(key = lambda x: x['_score'], reverse=True)\n",
    "    return {obj['_id']:obj['_score'] for obj in hits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('thing', 50),\n",
       " ('person', 21),\n",
       " ('organisation', 9),\n",
       " ('officeholder', 4),\n",
       " ('governmentagency', 3),\n",
       " ('academicjournal', 2),\n",
       " ('politician', 2),\n",
       " ('politicalparty', 2),\n",
       " ('non profitorganisation', 2),\n",
       " ('company', 1),\n",
       " ('saint', 1),\n",
       " ('museum', 1),\n",
       " ('writer', 1),\n",
       " ('ambassador', 1)]"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "internal_BM25(\"civil rights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'leadership conference on civil and human rights': 14.634237,\n",
       " 'civil rights commission puerto rico': 14.519391,\n",
       " 'lawyers committee for civil rights under law': 14.449045,\n",
       " 'chicano movement': 14.209293,\n",
       " 'civil rights act': 14.079039}"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "internal_BM25_score(\"civil rights\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_query(es, query, index=INDEX_NAME, field = 'abstract'):\n",
    "    \"\"\"Analyzes a query with respect to the relevant index. \n",
    "    \n",
    "    Arguments:\n",
    "        es: Elasticsearch object instance.\n",
    "        query: String of query terms.\n",
    "        field: The field with respect to which the query is analyzed. \n",
    "        index: Name of the index with respect to which the query is analyzed.  \n",
    "    \n",
    "    Returns:\n",
    "        A list of query terms that exist in the specified field among the documents in the index. \n",
    "    \"\"\"\n",
    "    tokens = es.indices.analyze(index=index, body={'text': query})['tokens']\n",
    "    query_terms = []\n",
    "    for t in sorted(tokens, key=lambda x: x['position']):\n",
    "        ## Use a boolean query to find at least one document that contains the term.\n",
    "        hits = es.search(index=index, body={'query': {'match': {field: t['token']}}}, \n",
    "                                   _source=False, size=1).get('hits', {}).get('hits', {})\n",
    "        doc_id = hits[0]['_id'] if len(hits) > 0 else None\n",
    "        if doc_id is None:\n",
    "            continue\n",
    "        query_terms.append(t['token'])\n",
    "    return query_terms\n",
    "\n",
    "def extract_features(query_terms, doc_id, es, index=INDEX_NAME):\n",
    "    \"\"\"Extracts query features, document features and query-document features of a query and document pair.\n",
    "    \n",
    "        Arguments:\n",
    "            query_terms: List of analyzed query terms.\n",
    "            doc_id: Document identifier of indexed document.\n",
    "            es: Elasticsearch object instance.\n",
    "            index: Name of relevant index on the running Elasticsearch service. \n",
    "            \n",
    "        Returns:\n",
    "            List of extracted feature values in a fixed order.\n",
    "    \"\"\"\n",
    "    feature_vect = []\n",
    "    # TODO\n",
    "    return feature_vect\n",
    "\n",
    "def prepare_ltr_training_data(es, k=100, amount=0, index=INDEX_NAME):\n",
    "    \"\"\"Prepares feature vectors and labels for query and document pairs found in the training data.\n",
    "    \n",
    "        Arguments:\n",
    "            query_ids: List of query IDs.\n",
    "            es: Elasticsearch object instance.\n",
    "            index: Name of relevant index on the running Elasticsearch service. \n",
    "            \n",
    "        Returns:\n",
    "            X: List of feature vectors extracted for each pair of query and retrieved or relevant document. \n",
    "            y: List of corresponding labels.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    progress, N = 0, len(training_queries)\n",
    "    for queryObject in training_queries.values():\n",
    "        query = analyze_query(es, queryObject['query'], index)\n",
    "        hits = es.search(\n",
    "            index=index, \n",
    "            _source=True, \n",
    "            size=k, \n",
    "            body={\"query\": {\"bool\": {\"must\": {\"match\": {\"abstract\": ' '.join(query)}}, \"must_not\": {\"match\": {\"instance\": \"thing\"}}}}}\n",
    "        )['hits']['hits']\n",
    "        for obj in hits:\n",
    "            dId, types = obj['_id'], obj['_source']['instance']\n",
    "            y.append((1 if (types in queryObject['type']) else 0))\n",
    "            X.append(extract_features(query, dId, es, index))\n",
    "\n",
    "        progress += 1\n",
    "        if (progress % 50) == 0:\n",
    "            print('Progress - {}/{} queries handled.'.format(progress, N))\n",
    "\n",
    "        if amount and (progress >= amount):\n",
    "            break\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def evaluate_baseline(es, k=100, amount=0, index=INDEX_NAME):\n",
    "    \"\"\"\n",
    "    Evaluate the BM25 baseline on our train queries.\n",
    "    \"\"\"\n",
    "    matches, progress, N = 0, 0, len(training_queries)\n",
    "    for queryObject in training_queries.values():\n",
    "        query = analyze_query(es, queryObject['query'], index)\n",
    "        hits = es.search(\n",
    "            index=index, \n",
    "            _source=True, \n",
    "            size=k, \n",
    "            body={\"query\": {\"bool\": {\"must\": {\"match\": {\"abstract\": ' '.join(query)}}, \"must_not\": {\"match\": {\"instance\": \"thing\"}}}}}\n",
    "        )['hits']['hits']\n",
    "        for obj in hits:\n",
    "            types = obj['_source']['instance']\n",
    "            if (types in queryObject['type']):\n",
    "                matches += 1\n",
    "                break\n",
    "\n",
    "        progress += 1\n",
    "        if (progress % 50) == 0:\n",
    "            print('Progress - {}/{} queries handled.'.format(progress, N))\n",
    "\n",
    "        if amount and (progress >= amount):\n",
    "            break\n",
    "\n",
    "    return (matches / progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Progress - 50/9557 queries handled.\n"
     ]
    }
   ],
   "source": [
    "x,y = prepare_ltr_training_data(es, amount=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Progress - 50/9557 queries handled.\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate_baseline(es, amount=80) # Eval. X queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy -> 0.65\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy ->', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'album', 'book', 'recordlabel', 'televisionshow'}"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "v = es.search(index=INDEX_NAME, body={\"query\": {\"bool\": {\"must\": {\"match\": {\"abstract\": \"who is george carlin\"}}, \"must_not\": {\"match\": {\"instance\": \"thing\"}}}}})['hits']['hits']\n",
    "set([x['_source']['instance'] for x in v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'album', 'book', 'recordlabel', 'televisionshow', 'thing'}"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "v = es.search(index=INDEX_NAME, body={\"query\": {\"bool\": {\"must\": {\"match\": {\"abstract\": \"who is george carlin\"}}}}})['hits']['hits']\n",
    "set([x['_source']['instance'] for x in v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}