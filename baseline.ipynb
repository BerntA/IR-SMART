{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import elasticsearch\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_TRAIN_FILEPATH = 'datasets\\DBpedia\\smarttask_dbpedia_train.json'\n",
    "QUERY_TEST_FILEPATH = 'datasets\\DBpedia\\smarttask_dbpedia_test_questions.json'\n",
    "\n",
    "INDEX_NAME = 'fasttest'\n",
    "FIELDS = ['abstract', 'subject', \"instance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\"that'll\", 'out', \"it's\", 'or', 'who', 'ourselves', 'them', 'itself', 'other', 'under', \"isn't\", 'me', 'don', 'which', 'your', 'why', 'will', \"you're\", 'a', 'than', 'she', 're', 'he', 'down', 'each', \"weren't\", 'being', 'won', 'most', 'yours', 'wasn', 'with', \"needn't\", 'd', 'off', 'those', 'had', 'you', 'once', 'of', \"wasn't\", 'after', 'the', \"doesn't\", 'there', 'while', 'they', 'their', 'these', 'is', 'be', 'have', 'until', \"hasn't\", 'mightn', 'wouldn', 'theirs', 'too', 'ours', \"won't\", 'him', \"you'd\", 'own', 'y', 'more', \"couldn't\", 'needn', 'whom', 'during', 'll', 'ain', 'i', \"don't\", 'such', 'only', \"haven't\", \"wouldn't\", 'then', 'o', 's', 'its', \"mustn't\", 'from', 'was', 'what', 'over', 'been', 'couldn', 'into', 't', 'can', 'now', 'isn', \"shan't\", 'himself', \"you've\", 'this', 'yourselves', 'his', 'do', 'how', 'are', 'her', 'through', 'doesn', 'very', 'up', 'because', 'some', 'my', 'at', \"shouldn't\", 'both', 'mustn', 'but', 'myself', 'yourself', 'an', 'and', 'as', 'am', 'does', 'again', 'few', 'shan', 'weren', 'having', 'that', 've', 'hasn', \"she's\", \"didn't\", 'nor', 'between', 'when', \"mightn't\", 'same', \"you'll\", 'below', 'any', 'shouldn', 'ma', 'for', 'm', 'has', 'all', 'if', 'in', 'just', 'our', 'haven', 'themselves', 'did', 'to', 'no', 'aren', 'doing', \"aren't\", 'above', 'were', 'where', 'further', 'about', \"should've\", 'didn', 'hers', 'on', 'not', 'should', 'by', 'herself', 'it', \"hadn't\", 'before', 'hadn', 'we', 'here', 'so', 'against'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'name': 'BERNTA-PC',\n",
       " 'cluster_name': 'elasticsearch',\n",
       " 'cluster_uuid': 'IP06yo9vScKZA1ZTb8R9HA',\n",
       " 'version': {'number': '7.9.2',\n",
       "  'build_flavor': 'default',\n",
       "  'build_type': 'zip',\n",
       "  'build_hash': 'd34da0ea4a966c4e49417f2da2f244e3e97b4e6e',\n",
       "  'build_date': '2020-09-23T00:45:33.626720Z',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '8.6.2',\n",
       "  'minimum_wire_compatibility_version': '6.8.0',\n",
       "  'minimum_index_compatibility_version': '6.0.0-beta1'},\n",
       " 'tagline': 'You Know, for Search'}"
      ]
     },
     "metadata": {},
     "execution_count": 160
    }
   ],
   "source": [
    "es = Elasticsearch()\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_queries(filepath):\n",
    "    \"\"\"\n",
    "    Load training queries from a file. \n",
    "    Returns a dictoinary with queryID as key and corresponding query, category and type.\n",
    "    \"\"\"\n",
    "    query_dicts = {}\n",
    "    with open(filepath, \"r\") as f:\n",
    "      queries = f.read()\n",
    "    \n",
    "    for query in json.loads(queries):\n",
    "        try:\n",
    "            query_dicts[query[\"id\"]] = {\"query\": query[\"question\"].lower(), \"category\": query[\"category\"].lower(), \"type\": ' '.join(query[\"type\"]).lower()}\n",
    "        except Exception as e:\n",
    "            # print(\"Query: {}\\n\\tThrew an exception: {}\\n\".format(query, e))\n",
    "            continue\n",
    "    return query_dicts\n",
    "\n",
    "def load_test_queries(filepath):\n",
    "    \"\"\"\n",
    "    Load test queries from a file.\n",
    "    Returns a dictionary with queryID as key, and corresponding query as a string.\n",
    "    \"\"\"\n",
    "    query_dicts = {}\n",
    "    with open(filepath, \"r\") as f:\n",
    "      queries = f.read()\n",
    "    \n",
    "    for query in json.loads(queries):\n",
    "        try:\n",
    "            query_dicts[query[\"id\"]] = {\"query\": query[\"question\"].lower()}\n",
    "        except Exception as e:\n",
    "            # print(\"Query: {}\\n\\tThrew an exception: {}\\n\".format(query, e))\n",
    "            continue\n",
    "    return query_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# training queries: 17254 \n\tExample key'dbpedia_17655' returns: {'query': 'what town is the birthplace of joseph greenberg?', 'category': 'resource', 'type': 'dbo:city dbo:settlement dbo:populatedplace dbo:place dbo:location'}\n# test queries: 4369 \n\tExample key'dbpedia_21099' contain: {'query': 'under which president did some politicians live in kensington?'}\n"
     ]
    }
   ],
   "source": [
    "training_queries = load_train_queries(QUERY_TRAIN_FILEPATH)\n",
    "test_queries = load_test_queries(QUERY_TEST_FILEPATH)\n",
    "\n",
    "print(\"# training queries:\", len(training_queries), \"\\n\\tExample key'dbpedia_17655' returns:\", training_queries['dbpedia_17655'])\n",
    "print(\"# test queries:\", len(test_queries), \"\\n\\tExample key'dbpedia_21099' contain:\", test_queries['dbpedia_21099'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Retrieval\n",
    "Implements Okapi BM25, uses the Elastic search inbuilt implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def internal_BM25(query, k = 100, field = 'abstract', index = INDEX_NAME):\n",
    "    \"\"\"\n",
    "    Perform baseline retrieval on a index using the inbuilt BM25 index\n",
    "\n",
    "    Arguments:\n",
    "        index: string\n",
    "        query: string, space separated terms\n",
    "        k: integer\n",
    "    \n",
    "    Returns:\n",
    "        List of k first entity IDs(string)\n",
    "    \"\"\"\n",
    "    hits = es.search(index=index, body={'query': {'match': {field: query}}}, _source=False, size=k).get('hits', {}).get('hits', {})\n",
    "    hits_ids = [obj['_id'] for obj in hits]\n",
    "    hits_types = [es.get(index=index, id=doc)[\"_source\"].get(\"instance\", \"Thing\") for doc in hits_ids]\n",
    "    return Counter([obj for obj in hits_types if len(obj) > 0]).most_common()\n",
    "    \n",
    "def internal_BM25_score(query, k = 100, field = 'abstract', index = INDEX_NAME):\n",
    "    \"\"\"\n",
    "    Perform baseline retrieval on a index using the inbuilt BM25 index\n",
    "\n",
    "    Arguments:\n",
    "        index: string\n",
    "        query: string, space separated terms\n",
    "        k: integer\n",
    "    \n",
    "    Returns:\n",
    "        List of k first entity IDs(string), and corresponding score(double)\n",
    "    \"\"\"\n",
    "    hits = es.search(index=index, body={'query': {'match': {field: query}}}, _source=False, size=k).get('hits', {}).get('hits', {})\n",
    "    hits.sort(key = lambda x: x['_score'], reverse=True)\n",
    "    return {obj['_id']:obj['_score'] for obj in hits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Person', 22),\n",
       " ('Thing', 12),\n",
       " ('Organisation', 7),\n",
       " ('OfficeHolder', 4),\n",
       " ('GovernmentAgency', 3),\n",
       " ('Politician', 2),\n",
       " ('AcademicJournal', 2),\n",
       " ('Non-ProfitOrganisation', 2),\n",
       " ('Company', 1),\n",
       " ('Museum', 1),\n",
       " ('Saint', 1),\n",
       " ('PoliticalParty', 1),\n",
       " ('Writer', 1),\n",
       " ('Ambassador', 1)]"
      ]
     },
     "metadata": {},
     "execution_count": 164
    }
   ],
   "source": [
    "internal_BM25(\"civil rights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Leadership Conference on Civil and Human Rights': 14.707036,\n",
       " 'Civil Rights Commission (Puerto Rico)': 14.573362,\n",
       " 'Lawyers Committee for Civil Rights Under Law': 14.516139,\n",
       " 'Chicano Movement': 14.269186,\n",
       " 'Civil Rights Act': 14.151209}"
      ]
     },
     "metadata": {},
     "execution_count": 165
    }
   ],
   "source": [
    "internal_BM25_score(\"civil rights\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_query(es, query, index=INDEX_NAME, field = 'abstract'):\n",
    "    \"\"\"Analyzes a query with respect to the relevant index. \n",
    "    \n",
    "    Arguments:\n",
    "        es: Elasticsearch object instance.\n",
    "        query: String of query terms.\n",
    "        field: The field with respect to which the query is analyzed. \n",
    "        index: Name of the index with respect to which the query is analyzed.  \n",
    "    \n",
    "    Returns:\n",
    "        A list of query terms that exist in the specified field among the documents in the index. \n",
    "    \"\"\"\n",
    "    tokens = es.indices.analyze(index=index, body={'text': query})['tokens']\n",
    "    query_terms = []\n",
    "    for t in sorted(tokens, key=lambda x: x['position']):\n",
    "        ## Use a boolean query to find at least one document that contains the term.\n",
    "        hits = es.search(index=index, body={'query': {'match': {field: t['token']}}}, \n",
    "                                   _source=False, size=1).get('hits', {}).get('hits', {})\n",
    "        doc_id = hits[0]['_id'] if len(hits) > 0 else None\n",
    "        if doc_id is None:\n",
    "            continue\n",
    "        query_terms.append(t['token'])\n",
    "    return query_terms\n",
    "\n",
    "def extract_features(query_terms, doc_id, es, index=INDEX_NAME):\n",
    "    \"\"\"Extracts query features, document features and query-document features of a query and document pair.\n",
    "    \n",
    "        Arguments:\n",
    "            query_terms: List of analyzed query terms.\n",
    "            doc_id: Document identifier of indexed document.\n",
    "            es: Elasticsearch object instance.\n",
    "            index: Name of relevant index on the running Elasticsearch service. \n",
    "            \n",
    "        Returns:\n",
    "            List of extracted feature values in a fixed order.\n",
    "    \"\"\"\n",
    "    feature_vect = []\n",
    "    # TODO\n",
    "    return feature_vect\n",
    "\n",
    "def prepare_ltr_training_data(es, k=100, amount=0, index=INDEX_NAME):\n",
    "    \"\"\"Prepares feature vectors and labels for query and document pairs found in the training data.\n",
    "    \n",
    "        Arguments:\n",
    "            query_ids: List of query IDs.\n",
    "            es: Elasticsearch object instance.\n",
    "            index: Name of relevant index on the running Elasticsearch service. \n",
    "            \n",
    "        Returns:\n",
    "            X: List of feature vectors extracted for each pair of query and retrieved or relevant document. \n",
    "            y: List of corresponding labels.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    progress, N = 0, len(training_queries)\n",
    "    for queryObject in training_queries.values():\n",
    "        # Skip queries without a dbo: type.\n",
    "        if not 'dbo:' in queryObject['type']:\n",
    "            continue\n",
    "\n",
    "        query = analyze_query(es, queryObject['query'], index)\n",
    "        hits = es.search(index=index, q=' '.join(query), _source=True, size=k)['hits']['hits']\n",
    "        for obj in hits:\n",
    "            dId, types = obj['_id'], obj['_source']['instance'].lower()\n",
    "            if len(types) == 0:\n",
    "                types = 'thing' # Default\n",
    "\n",
    "            y.append((1 if (types in queryObject['type']) else 0))\n",
    "            X.append(extract_features(query, dId, es, index))\n",
    "\n",
    "            #if types != 'thing':\n",
    "            #print('Query:', ' '.join(query), ', Expected type:', queryObject['type'], ', Returned type:' , types)\n",
    "\n",
    "        progress += 1\n",
    "        if (progress % 50) == 0:\n",
    "            print('Progress - {}/{} queries handled.'.format(progress, N))\n",
    "\n",
    "        if amount and (progress >= amount):\n",
    "            break\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def evaluate_baseline(es, k=100, amount=0, index=INDEX_NAME):\n",
    "    \"\"\"\n",
    "    Evaluate the BM25 baseline on our train queries.\n",
    "    \"\"\"\n",
    "    matches, progress, N = 0, 0, len(training_queries)\n",
    "    for queryObject in training_queries.values():\n",
    "        # Skip queries without a dbo: type.\n",
    "        if not 'dbo:' in queryObject['type']:\n",
    "            continue\n",
    "\n",
    "        query = analyze_query(es, queryObject['query'], index)\n",
    "        hits = es.search(index=index, q=' '.join(query), _source=True, size=k)['hits']['hits']\n",
    "        for obj in hits:\n",
    "            types = obj['_source']['instance'].lower()\n",
    "            if len(types) == 0:\n",
    "                types = 'thing' # Default\n",
    "\n",
    "            if (types in queryObject['type']):\n",
    "                matches += 1\n",
    "                break\n",
    "\n",
    "        progress += 1\n",
    "        if (progress % 50) == 0:\n",
    "            print('Progress - {}/{} queries handled.'.format(progress, N))\n",
    "\n",
    "        if amount and (progress >= amount):\n",
    "            break\n",
    "\n",
    "    return (matches / progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Progress - 50/17254 queries handled.\n"
     ]
    }
   ],
   "source": [
    "x,y = prepare_ltr_training_data(es, k=100, amount=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Progress - 50/17254 queries handled.\n",
      "Progress - 100/17254 queries handled.\n",
      "Progress - 150/17254 queries handled.\n",
      "Progress - 200/17254 queries handled.\n",
      "Progress - 250/17254 queries handled.\n",
      "Progress - 300/17254 queries handled.\n",
      "Progress - 350/17254 queries handled.\n",
      "Progress - 400/17254 queries handled.\n",
      "Progress - 450/17254 queries handled.\n",
      "Progress - 500/17254 queries handled.\n",
      "Progress - 550/17254 queries handled.\n",
      "Progress - 600/17254 queries handled.\n",
      "Progress - 650/17254 queries handled.\n",
      "Progress - 700/17254 queries handled.\n",
      "Progress - 750/17254 queries handled.\n",
      "Progress - 800/17254 queries handled.\n",
      "Progress - 850/17254 queries handled.\n",
      "Progress - 900/17254 queries handled.\n",
      "Progress - 950/17254 queries handled.\n",
      "Progress - 1000/17254 queries handled.\n",
      "Progress - 1050/17254 queries handled.\n",
      "Progress - 1100/17254 queries handled.\n",
      "Progress - 1150/17254 queries handled.\n",
      "Progress - 1200/17254 queries handled.\n",
      "Progress - 1250/17254 queries handled.\n",
      "Progress - 1300/17254 queries handled.\n",
      "Progress - 1350/17254 queries handled.\n",
      "Progress - 1400/17254 queries handled.\n",
      "Progress - 1450/17254 queries handled.\n",
      "Progress - 1500/17254 queries handled.\n",
      "Progress - 1550/17254 queries handled.\n",
      "Progress - 1600/17254 queries handled.\n",
      "Progress - 1650/17254 queries handled.\n",
      "Progress - 1700/17254 queries handled.\n",
      "Progress - 1750/17254 queries handled.\n",
      "Progress - 1800/17254 queries handled.\n",
      "Progress - 1850/17254 queries handled.\n",
      "Progress - 1900/17254 queries handled.\n",
      "Progress - 1950/17254 queries handled.\n",
      "Progress - 2000/17254 queries handled.\n",
      "Progress - 2050/17254 queries handled.\n",
      "Progress - 2100/17254 queries handled.\n",
      "Progress - 2150/17254 queries handled.\n",
      "Progress - 2200/17254 queries handled.\n",
      "Progress - 2250/17254 queries handled.\n",
      "Progress - 2300/17254 queries handled.\n",
      "Progress - 2350/17254 queries handled.\n",
      "Progress - 2400/17254 queries handled.\n",
      "Progress - 2450/17254 queries handled.\n",
      "Progress - 2500/17254 queries handled.\n",
      "Progress - 2550/17254 queries handled.\n",
      "Progress - 2600/17254 queries handled.\n",
      "Progress - 2650/17254 queries handled.\n",
      "Progress - 2700/17254 queries handled.\n",
      "Progress - 2750/17254 queries handled.\n",
      "Progress - 2800/17254 queries handled.\n",
      "Progress - 2850/17254 queries handled.\n",
      "Progress - 2900/17254 queries handled.\n",
      "Progress - 2950/17254 queries handled.\n",
      "Progress - 3000/17254 queries handled.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7293333333333333"
      ]
     },
     "metadata": {},
     "execution_count": 197
    }
   ],
   "source": [
    "acc = evaluate_baseline(es, k=200, amount=3000) # Eval. X queries.\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}