{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import elasticsearch\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_TRAIN_FILEPATH = 'datasets\\DBpedia\\smarttask_dbpedia_train.json'\n",
    "QUERY_TEST_FILEPATH = 'datasets\\DBpedia\\smarttask_dbpedia_test_questions.json'\n",
    "\n",
    "INDEX_NAME = 'fasttest'\n",
    "FIELDS = ['abstract', 'subject', \"instance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'by', 'not', 'they', 'there', 'in', 'then', 'with', 'was', 'their', 'be', 'on', 'and', 'it', 'of', 'an', 'are', 'into', 'that', 'these', 'if', 'for', 'to', 'but', 'no', 'the', 'this', 'is', 'such', 'as', 'at', 'or', 'a', 'will'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(['a', 'an', 'and', 'are', 'as', 'at', 'be', 'but', 'by', 'for', 'if', 'in', 'into', 'is', 'it', 'no', 'not', 'of', 'on', 'or', 'such', 'that', 'the', 'their', 'then', 'there', 'these', 'they', 'this', 'to', 'was', 'will', 'with'])\n",
    "print(stop_words) # Default in ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = string.punctuation.replace(\"'\", \"\")\n",
    "def preprocess(text):\n",
    "    text = text.strip().lower().translate(str.maketrans('', '', punctuation)) # Remove punctuation stuff.\n",
    "    text = re.sub('\\s\\s+', ' ', text).split(' ') # Replace consequtive whitespace with a single space.\n",
    "    return ' '.join([v for v in text if not v in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'BERNTA-PC',\n",
       " 'cluster_name': 'elasticsearch',\n",
       " 'cluster_uuid': 'IP06yo9vScKZA1ZTb8R9HA',\n",
       " 'version': {'number': '7.9.2',\n",
       "  'build_flavor': 'default',\n",
       "  'build_type': 'zip',\n",
       "  'build_hash': 'd34da0ea4a966c4e49417f2da2f244e3e97b4e6e',\n",
       "  'build_date': '2020-09-23T00:45:33.626720Z',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '8.6.2',\n",
       "  'minimum_wire_compatibility_version': '6.8.0',\n",
       "  'minimum_index_compatibility_version': '6.0.0-beta1'},\n",
       " 'tagline': 'You Know, for Search'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = Elasticsearch()\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_queries(filepath):\n",
    "    \"\"\"\n",
    "    Load training queries from a file. \n",
    "    Returns a dictoinary with queryID as key and corresponding query, category and type.\n",
    "    \"\"\"\n",
    "    query_dicts = {}\n",
    "    with open(filepath, \"r\") as f:\n",
    "      queries = f.read()\n",
    "    \n",
    "    for query in json.loads(queries):\n",
    "        try:\n",
    "            qID, qText, qCat, qType = query[\"id\"].lower(), query[\"question\"].lower(), query[\"category\"].lower(), ' '.join(query[\"type\"]).lower()\n",
    "            if not 'dbo:' in qType: # Skip queries without a dbo: type.\n",
    "                continue\n",
    "            query_dicts[qID] = {\"query\": preprocess(qText), \"category\": qCat, \"type\": qType}\n",
    "        except Exception as e:\n",
    "            # print(\"Query: {}\\n\\tThrew an exception: {}\\n\".format(query, e))\n",
    "            continue\n",
    "    return query_dicts\n",
    "\n",
    "def load_test_queries(filepath):\n",
    "    \"\"\"\n",
    "    Load test queries from a file.\n",
    "    Returns a dictionary with queryID as key, and corresponding query as a string.\n",
    "    \"\"\"\n",
    "    query_dicts = {}\n",
    "    with open(filepath, \"r\") as f:\n",
    "      queries = f.read()\n",
    "    \n",
    "    for query in json.loads(queries):\n",
    "        try:\n",
    "            query_dicts[query[\"id\"].lower()] = {\"query\": preprocess(query[\"question\"].lower())}\n",
    "        except Exception as e:\n",
    "            # print(\"Query: {}\\n\\tThrew an exception: {}\\n\".format(query, e))\n",
    "            continue\n",
    "    return query_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training queries: 9557 \n",
      "\tExample key'dbpedia_17655' returns: {'query': 'what town birthplace joseph greenberg', 'category': 'resource', 'type': 'dbo:city dbo:settlement dbo:populatedplace dbo:place dbo:location'}\n",
      "# test queries: 4369 \n",
      "\tExample key'dbpedia_21099' contain: {'query': 'under which president did some politicians live kensington'}\n"
     ]
    }
   ],
   "source": [
    "training_queries = load_train_queries(QUERY_TRAIN_FILEPATH)\n",
    "test_queries = load_test_queries(QUERY_TEST_FILEPATH)\n",
    "\n",
    "print(\"# training queries:\", len(training_queries), \"\\n\\tExample key'dbpedia_17655' returns:\", training_queries['dbpedia_17655'])\n",
    "print(\"# test queries:\", len(test_queries), \"\\n\\tExample key'dbpedia_21099' contain:\", test_queries['dbpedia_21099'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Retrieval\n",
    "Implements Okapi BM25, uses the Elastic search inbuilt implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def internal_BM25(query, k = 100, field = 'abstract', index = INDEX_NAME):\n",
    "    \"\"\"\n",
    "    Perform baseline retrieval on a index using the inbuilt BM25 index\n",
    "\n",
    "    Arguments:\n",
    "        index: string\n",
    "        query: string, space separated terms\n",
    "        k: integer\n",
    "    \n",
    "    Returns:\n",
    "        List of k first entity IDs(string)\n",
    "    \"\"\"\n",
    "    hits = es.search(index=index, body={'query': {'match': {field: query}}}, _source=False, size=k).get('hits', {}).get('hits', {})\n",
    "    hits_ids = [obj['_id'] for obj in hits]\n",
    "    hits_types = [es.get(index=index, id=doc)[\"_source\"].get(\"instance\", \"Thing\") for doc in hits_ids]\n",
    "    return Counter([obj for obj in hits_types if len(obj) > 0]).most_common()\n",
    "    \n",
    "def internal_BM25_score(query, k = 100, field = 'abstract', index = INDEX_NAME):\n",
    "    \"\"\"\n",
    "    Perform baseline retrieval on a index using the inbuilt BM25 index\n",
    "\n",
    "    Arguments:\n",
    "        index: string\n",
    "        query: string, space separated terms\n",
    "        k: integer\n",
    "    \n",
    "    Returns:\n",
    "        List of k first entity IDs(string), and corresponding score(double)\n",
    "    \"\"\"\n",
    "    hits = es.search(index=index, body={'query': {'match': {field: query}}}, _source=False, size=k).get('hits', {}).get('hits', {})\n",
    "    hits.sort(key = lambda x: x['_score'], reverse=True)\n",
    "    return {obj['_id']:obj['_score'] for obj in hits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Person', 22),\n",
       " ('Thing', 12),\n",
       " ('Organisation', 7),\n",
       " ('OfficeHolder', 4),\n",
       " ('GovernmentAgency', 3),\n",
       " ('Politician', 2),\n",
       " ('AcademicJournal', 2),\n",
       " ('Non-ProfitOrganisation', 2),\n",
       " ('Company', 1),\n",
       " ('Museum', 1),\n",
       " ('Saint', 1),\n",
       " ('PoliticalParty', 1),\n",
       " ('Writer', 1),\n",
       " ('Ambassador', 1)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "internal_BM25(\"civil rights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Leadership Conference on Civil and Human Rights': 14.707036,\n",
       " 'Civil Rights Commission (Puerto Rico)': 14.573362,\n",
       " 'Lawyers Committee for Civil Rights Under Law': 14.516139,\n",
       " 'Chicano Movement': 14.269186,\n",
       " 'Civil Rights Act': 14.151209}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "internal_BM25_score(\"civil rights\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_query(es, query, index=INDEX_NAME, field = 'abstract'):\n",
    "    \"\"\"Analyzes a query with respect to the relevant index. \n",
    "    \n",
    "    Arguments:\n",
    "        es: Elasticsearch object instance.\n",
    "        query: String of query terms.\n",
    "        field: The field with respect to which the query is analyzed. \n",
    "        index: Name of the index with respect to which the query is analyzed.  \n",
    "    \n",
    "    Returns:\n",
    "        A list of query terms that exist in the specified field among the documents in the index. \n",
    "    \"\"\"\n",
    "    tokens = es.indices.analyze(index=index, body={'text': query})['tokens']\n",
    "    query_terms = []\n",
    "    for t in sorted(tokens, key=lambda x: x['position']):\n",
    "        ## Use a boolean query to find at least one document that contains the term.\n",
    "        hits = es.search(index=index, body={'query': {'match': {field: t['token']}}}, \n",
    "                                   _source=False, size=1).get('hits', {}).get('hits', {})\n",
    "        doc_id = hits[0]['_id'] if len(hits) > 0 else None\n",
    "        if doc_id is None:\n",
    "            continue\n",
    "        query_terms.append(t['token'])\n",
    "    return query_terms\n",
    "\n",
    "def extract_features(query_terms, doc_id, es, index=INDEX_NAME):\n",
    "    \"\"\"Extracts query features, document features and query-document features of a query and document pair.\n",
    "    \n",
    "        Arguments:\n",
    "            query_terms: List of analyzed query terms.\n",
    "            doc_id: Document identifier of indexed document.\n",
    "            es: Elasticsearch object instance.\n",
    "            index: Name of relevant index on the running Elasticsearch service. \n",
    "            \n",
    "        Returns:\n",
    "            List of extracted feature values in a fixed order.\n",
    "    \"\"\"\n",
    "    feature_vect = []\n",
    "    # TODO\n",
    "    return feature_vect\n",
    "\n",
    "def prepare_ltr_training_data(es, k=100, amount=0, index=INDEX_NAME):\n",
    "    \"\"\"Prepares feature vectors and labels for query and document pairs found in the training data.\n",
    "    \n",
    "        Arguments:\n",
    "            query_ids: List of query IDs.\n",
    "            es: Elasticsearch object instance.\n",
    "            index: Name of relevant index on the running Elasticsearch service. \n",
    "            \n",
    "        Returns:\n",
    "            X: List of feature vectors extracted for each pair of query and retrieved or relevant document. \n",
    "            y: List of corresponding labels.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    progress, N = 0, len(training_queries)\n",
    "    for queryObject in training_queries.values():\n",
    "        query = analyze_query(es, queryObject['query'], index)\n",
    "        hits = es.search(index=index, q=' '.join(query), _source=True, size=k)['hits']['hits']\n",
    "        for obj in hits:\n",
    "            dId, types = obj['_id'], obj['_source']['instance'].lower()\n",
    "            if len(types) == 0:\n",
    "                types = 'thing' # Default\n",
    "\n",
    "            y.append((1 if (types in queryObject['type']) else 0))\n",
    "            X.append(extract_features(query, dId, es, index))\n",
    "\n",
    "            #if types != 'thing':\n",
    "            #print('Query:', ' '.join(query), ', Expected type:', queryObject['type'], ', Returned type:' , types)\n",
    "\n",
    "        progress += 1\n",
    "        if (progress % 50) == 0:\n",
    "            print('Progress - {}/{} queries handled.'.format(progress, N))\n",
    "\n",
    "        if amount and (progress >= amount):\n",
    "            break\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def evaluate_baseline(es, k=100, amount=0, index=INDEX_NAME):\n",
    "    \"\"\"\n",
    "    Evaluate the BM25 baseline on our train queries.\n",
    "    \"\"\"\n",
    "    matches, progress, N = 0, 0, len(training_queries)\n",
    "    for queryObject in training_queries.values():\n",
    "        query = analyze_query(es, queryObject['query'], index)\n",
    "        hits = es.search(index=index, q=' '.join(query), _source=True, size=k)['hits']['hits']\n",
    "        for obj in hits:\n",
    "            types = obj['_source']['instance'].lower()\n",
    "            if len(types) == 0:\n",
    "                types = 'thing' # Default\n",
    "\n",
    "            if (types in queryObject['type']):\n",
    "                matches += 1\n",
    "                break\n",
    "\n",
    "        progress += 1\n",
    "        if (progress % 50) == 0:\n",
    "            print('Progress - {}/{} queries handled.'.format(progress, N))\n",
    "\n",
    "        if amount and (progress >= amount):\n",
    "            break\n",
    "\n",
    "    return (matches / progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress - 50/17254 queries handled.\n"
     ]
    }
   ],
   "source": [
    "x,y = prepare_ltr_training_data(es, k=100, amount=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress - 50/9557 queries handled.\n",
      "Progress - 100/9557 queries handled.\n",
      "Progress - 150/9557 queries handled.\n",
      "Progress - 200/9557 queries handled.\n",
      "Progress - 250/9557 queries handled.\n",
      "Progress - 300/9557 queries handled.\n",
      "Progress - 350/9557 queries handled.\n",
      "Progress - 400/9557 queries handled.\n",
      "Progress - 450/9557 queries handled.\n",
      "Progress - 500/9557 queries handled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.724"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = evaluate_baseline(es, k=200, amount=500) # Eval. X queries.\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
