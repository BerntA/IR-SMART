{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import elasticsearch\n",
    "import string\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_TRAIN_FILEPATH = 'datasets\\DBpedia\\smarttask_dbpedia_train.json'\n",
    "QUERY_TEST_FILEPATH = 'datasets\\DBpedia\\smarttask_dbpedia_test_questions.json'\n",
    "INDEX_NAME = 'smart'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'to', 'an', 'a', 'as', 'but', 'are', 'then', 'is', 'with', 'the', 'at', 'these', 'for', 'not', 'and', 'this', 'was', 'will', 'no', 'that', 'they', 'or', 'in', 'of', 'be', 'by', 'into', 'if', 'it', 'on', 'there', 'their', 'such'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(['a', 'an', 'and', 'are', 'as', 'at', 'be', 'but', 'by', 'for', 'if', 'in', 'into', 'is', 'it', 'no', 'not', 'of', 'on', 'or', 'such', 'that', 'the', 'their', 'then', 'there', 'these', 'they', 'this', 'to', 'was', 'will', 'with'])\n",
    "print(stop_words) # Default in ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.strip().lower()\n",
    "    text = text.replace('_', ' ').replace('-', ' ')\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # Remove punctuation stuff.\n",
    "    text = re.sub('\\s\\s+', ' ', text).split(' ') # Replace consequtive whitespace with a single space.\n",
    "    return ' '.join([v for v in text if not v in stop_words]).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec - Convert GloVe to Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = datapath(os.getcwd()+'/datasets/gensim/gensim.6B.100d.txt')\n",
    "def convertGloveToGensim(target, output):\n",
    "    _ = glove2word2vec(datapath(os.getcwd()+target), datapath(os.getcwd()+output))\n",
    "\n",
    "#convertGloveToGensim('/datasets/glove/glove.6B.50d.txt', '/datasets/gensim/gensim.6B.50d.txt')\n",
    "#convertGloveToGensim('/datasets/glove/glove.6B.100d.txt', '/datasets/gensim/gensim.6B.100d.txt')\n",
    "#convertGloveToGensim('/datasets/glove/glove.6B.200d.txt', '/datasets/gensim/gensim.6B.200d.txt')\n",
    "#convertGloveToGensim('/datasets/glove/glove.6B.300d.txt', '/datasets/gensim/gensim.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'ULTIMECIA',\n",
       " 'cluster_name': 'elasticsearch',\n",
       " 'cluster_uuid': 'MHYEAbcOS_i6ybp0d4NE2A',\n",
       " 'version': {'number': '7.9.1',\n",
       "  'build_flavor': 'default',\n",
       "  'build_type': 'zip',\n",
       "  'build_hash': '083627f112ba94dffc1232e8b42b73492789ef91',\n",
       "  'build_date': '2020-09-01T21:22:21.964974Z',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '8.6.2',\n",
       "  'minimum_wire_compatibility_version': '6.8.0',\n",
       "  'minimum_index_compatibility_version': '6.0.0-beta1'},\n",
       " 'tagline': 'You Know, for Search'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = Elasticsearch()\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs: 4926217\n"
     ]
    }
   ],
   "source": [
    "es.indices.refresh(INDEX_NAME)\n",
    "count = es.cat.count(INDEX_NAME, params={\"format\": \"json\"})\n",
    "print('Docs:', int(count[0]['count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_queries(filepath):\n",
    "    \"\"\"\n",
    "    Load training queries from a file. \n",
    "    Returns a dictoinary with queryID as key and corresponding query, category and type.\n",
    "    \"\"\"\n",
    "    query_dicts = {}\n",
    "    with open(filepath, \"r\") as f:\n",
    "      queries = f.read()\n",
    "    \n",
    "    for query in json.loads(queries):\n",
    "        try:\n",
    "            qID, qText, qCat, qType = query[\"id\"].lower(), query[\"question\"].lower(), query[\"category\"].lower(), ' '.join(query[\"type\"]).lower()\n",
    "            if not 'dbo:' in qType: # Skip queries without a dbo: type.\n",
    "                continue\n",
    "            query_dicts[qID] = {\"query\": preprocess(qText), \"category\": qCat, \"type\": qType.replace('dbo:', '')}\n",
    "        except Exception as e:\n",
    "            # print(\"Query: {}\\n\\tThrew an exception: {}\\n\".format(query, e))\n",
    "            continue\n",
    "    return query_dicts\n",
    "\n",
    "def load_test_queries(filepath):\n",
    "    \"\"\"\n",
    "    Load test queries from a file.\n",
    "    Returns a dictionary with queryID as key, and corresponding query as a string.\n",
    "    \"\"\"\n",
    "    query_dicts = {}\n",
    "    with open(filepath, \"r\") as f:\n",
    "      queries = f.read()\n",
    "    \n",
    "    for query in json.loads(queries):\n",
    "        try:\n",
    "            query_dicts[query[\"id\"].lower()] = {\"query\": preprocess(query[\"question\"].lower())}\n",
    "        except Exception as e:\n",
    "            # print(\"Query: {}\\n\\tThrew an exception: {}\\n\".format(query, e))\n",
    "            continue\n",
    "    return query_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training queries: 9557 \n",
      "\tExample key'dbpedia_17655' returns: {'query': 'what town birthplace joseph greenberg', 'category': 'resource', 'type': 'city settlement populatedplace place location'}\n",
      "# test queries: 4369 \n",
      "\tExample key'dbpedia_21099' contain: {'query': 'under which president did some politicians live kensington'}\n"
     ]
    }
   ],
   "source": [
    "training_queries = load_train_queries(QUERY_TRAIN_FILEPATH)\n",
    "test_queries = load_test_queries(QUERY_TEST_FILEPATH)\n",
    "\n",
    "print(\"# training queries:\", len(training_queries), \"\\n\\tExample key'dbpedia_17655' returns:\", training_queries['dbpedia_17655'])\n",
    "print(\"# test queries:\", len(test_queries), \"\\n\\tExample key'dbpedia_21099' contain:\", test_queries['dbpedia_21099'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load evaluation types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDBPediaTypes():\n",
    "    kv = {}\n",
    "    max_depth = 0\n",
    "    with open('./evaluation/dbpedia/dbpedia_types.tsv', 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i == 0: # Skip header\n",
    "                continue\n",
    "            line = line.strip().lower().split('\\t')\n",
    "            if len(line) != 3:\n",
    "                continue\n",
    "            type_name, depth, parent_type = line[0].split(':')[-1], int(line[1]), line[-1].split(':')[-1]\n",
    "            if (len(type_name) == 0) or (len(parent_type) == 0):\n",
    "                continue\n",
    "            kv[type_name] = {'depth':depth, 'parent':parent_type}\n",
    "            max_depth = max(depth, max_depth)\n",
    "    return kv, max_depth\n",
    "\n",
    "def getTypeHierarchy(kv, items, target):\n",
    "    if not target in kv:\n",
    "        return\n",
    "    typeName, typeDepth = target, kv[target]['depth']\n",
    "    items.append(typeName)\n",
    "    getTypeHierarchy(kv, items, kv[target]['parent'])\n",
    "\n",
    "def buildDBPediaTypeHierarchy(kv, target):\n",
    "    items = [] # List of types, representing the hierarchy of the types related to the target.\n",
    "    getTypeHierarchy(kv, items, target)\n",
    "    return items[::-1] # Reverse the order to return the correct hierarchy where the first item = top level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['basketballleague', 'naturalevent', 'province', 'lunarcrater'] Max Depth 7\n"
     ]
    }
   ],
   "source": [
    "type_hierarchy, max_depth = loadDBPediaTypes()\n",
    "print(list(type_hierarchy.keys())[:4], 'Max Depth', max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['work', 'writtenwork', 'comic']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buildDBPediaTypeHierarchy(type_hierarchy, 'comic') # Example hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Retrieval\n",
    "Implements Okapi BM25, uses the Elastic search inbuilt implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def internal_BM25(query, k = 100, field = 'abstract', index = INDEX_NAME):\n",
    "    \"\"\"\n",
    "    Perform baseline retrieval on a index using the inbuilt BM25 index\n",
    "\n",
    "    Arguments:\n",
    "        index: string\n",
    "        query: string, space separated terms\n",
    "        k: integer\n",
    "    \n",
    "    Returns:\n",
    "        List of k first entity IDs(string)\n",
    "    \"\"\"\n",
    "    hits = es.search(index=index, body={'query': {'match': {field: query}}}, _source=False, size=k).get('hits', {}).get('hits', {})\n",
    "    hits_ids = [obj['_id'] for obj in hits]\n",
    "    hits_types = [es.get(index=index, id=doc)[\"_source\"].get(\"instance\", \"thing\") for doc in hits_ids]\n",
    "    return Counter([obj for obj in hits_types if len(obj) > 0]).most_common()\n",
    "    \n",
    "def internal_BM25_score(query, k = 100, field = 'abstract', index = INDEX_NAME):\n",
    "    \"\"\"\n",
    "    Perform baseline retrieval on a index using the inbuilt BM25 index\n",
    "\n",
    "    Arguments:\n",
    "        index: string\n",
    "        query: string, space separated terms\n",
    "        k: integer\n",
    "    \n",
    "    Returns:\n",
    "        List of k first entity IDs(string), and corresponding score(double)\n",
    "    \"\"\"\n",
    "    hits = es.search(index=index, body={'query': {'match': {field: query}}}, _source=False, size=k).get('hits', {}).get('hits', {})\n",
    "    hits.sort(key = lambda x: x['_score'], reverse=True)\n",
    "    return {obj['_id']:obj['_score'] for obj in hits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thing', 49),\n",
       " ('person', 22),\n",
       " ('organisation', 9),\n",
       " ('officeholder', 4),\n",
       " ('governmentagency', 3),\n",
       " ('politician', 2),\n",
       " ('academicjournal', 2),\n",
       " ('politicalparty', 2),\n",
       " ('non profitorganisation', 2),\n",
       " ('company', 1),\n",
       " ('museum', 1),\n",
       " ('saint', 1),\n",
       " ('writer', 1),\n",
       " ('ambassador', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "internal_BM25(\"civil rights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'leadership conference on civil and human rights': 14.680483,\n",
       " 'civil rights commission puerto rico': 14.557869,\n",
       " 'lawyers committee for civil rights under law': 14.492786,\n",
       " 'chicano movement': 14.249865,\n",
       " 'civil rights act': 14.124385}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "internal_BM25_score(\"civil rights\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_query(es, query, index=INDEX_NAME, field = 'abstract'):\n",
    "    \"\"\"Analyzes a query with respect to the relevant index. \n",
    "    \n",
    "    Arguments:\n",
    "        es: Elasticsearch object instance.\n",
    "        query: String of query terms.\n",
    "        field: The field with respect to which the query is analyzed. \n",
    "        index: Name of the index with respect to which the query is analyzed.  \n",
    "    \n",
    "    Returns:\n",
    "        A list of query terms that exist in the specified field among the documents in the index. \n",
    "    \"\"\"\n",
    "    tokens = es.indices.analyze(index=index, body={'text': query})['tokens']\n",
    "    query_terms = []\n",
    "    for t in sorted(tokens, key=lambda x: x['position']):\n",
    "        ## Use a boolean query to find at least one document that contains the term.\n",
    "        hits = es.search(index=index, body={'query': {'match': {field: t['token']}}}, \n",
    "                                   _source=False, size=1).get('hits', {}).get('hits', {})\n",
    "        doc_id = hits[0]['_id'] if len(hits) > 0 else None\n",
    "        if doc_id is None:\n",
    "            continue\n",
    "        query_terms.append(t['token'])\n",
    "    return query_terms\n",
    "\n",
    "def evaluate_baseline(es, amount=0, index=INDEX_NAME):\n",
    "    \"\"\"\n",
    "    Evaluate the BM25 baseline on our train queries.\n",
    "    \"\"\"\n",
    "    progress, N = 0, len(training_queries)\n",
    "    results = {}\n",
    "    for qId, queryObject in training_queries.items():\n",
    "        query = analyze_query(es, queryObject['query'], index)\n",
    "        hits = es.search(index=index, _source=True, size=10,\n",
    "            body={\"query\": {\"bool\": {\"must\": {\"match\": {\"abstract\": ' '.join(query)}}, \"must_not\": {\"match\": {\"instance\": \"thing\"}}}}}\n",
    "        )['hits']['hits']\n",
    "        hits_types = [obj['_source']['instance'] for obj in hits]\n",
    "        results[qId] = {\n",
    "            'type': hits_types,\n",
    "            'category': 'resource',\n",
    "            'match': max([(1 if (t in queryObject['type']) else 0) for t in hits_types]) # Yes / No was there an explicit match?\n",
    "        }\n",
    "\n",
    "        progress += 1\n",
    "        if (progress % 50) == 0:\n",
    "            print('Progress - {}/{} queries handled.'.format(progress, N))\n",
    "\n",
    "        if amount and (progress >= amount):\n",
    "            break\n",
    "\n",
    "    return results\n",
    "\n",
    "def tokens_to_vec(tokens, model):\n",
    "    \"\"\"\n",
    "    Convert a list of tokens to some word 2 vec representation which \n",
    "    conforms to our model.\n",
    "    \"\"\"\n",
    "    size = model.wv.vectors.shape[1]\n",
    "    if len(tokens) == 0:\n",
    "        return np.zeros(size)        \n",
    "    embeddings = []\n",
    "    for v in tokens:\n",
    "        embeddings.append((model.wv.word_vec(v) if (v in model.wv.vocab) else np.random.rand(size)))\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "def evaluate_advanced(es, model, k=1000, amount=0, index=INDEX_NAME):\n",
    "    \"\"\"\n",
    "    Evaluate our advanced method, re-rank the documents using word2vec.\n",
    "    We are using pre-trained embeddings. Convert each query and related doc to word2vec format,\n",
    "    compare the similarity and re-rank the entries.\n",
    "    \"\"\"\n",
    "    progress, N = 0, len(training_queries)\n",
    "    results = {}\n",
    "    for qId, queryObject in training_queries.items():\n",
    "        query = analyze_query(es, queryObject['query'], index)\n",
    "        hits = es.search(index=index, _source=True, size=k, \n",
    "            body={\"query\": {\"bool\": {\"must\": {\"match\": {\"abstract\": ' '.join(query)}}, \"must_not\": {\"match\": {\"instance\": \"thing\"}}}}}\n",
    "        )['hits']['hits']\n",
    "        queryEmbedding = tokens_to_vec(query, model).reshape(1, -1)\n",
    "        rerank = []\n",
    "        for obj in hits:\n",
    "            docEmbedding = tokens_to_vec(obj['_source']['abstract'].split(), model).reshape(1, -1)\n",
    "            sim = cosine_similarity(queryEmbedding, docEmbedding).item()\n",
    "            rerank.append((obj['_source']['instance'], sim))\n",
    "            \n",
    "        rerank.sort(key=lambda x:x[-1], reverse=True) # Re-rank the initial hits using our word2vec mdl.\n",
    "        results[qId] = {\n",
    "            'type': [v for v,_ in rerank[:10]],\n",
    "            'category': 'resource',\n",
    "            'match': max([(1 if (t in queryObject['type']) else 0) for t,_ in rerank[:10]]) # Yes / No was there an explicit match?\n",
    "        }\n",
    "\n",
    "        progress += 1\n",
    "        if (progress % 50) == 0:\n",
    "            print('Progress - {}/{} queries handled.'.format(progress, N))\n",
    "\n",
    "        if amount and (progress >= amount):\n",
    "            break\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg(gains, k=5):\n",
    "    \"\"\"\n",
    "    Computes DCG for a given ranking.\n",
    "    Traditional DCG formula: DCG_k = sum_{i=1}^k gain_i / log_2(i+1).\n",
    "    \"\"\"\n",
    "    dcg = 0\n",
    "    for i in range(0, min(k, len(gains))):\n",
    "        dcg += gains[i] / math.log(i + 2, 2)\n",
    "    return dcg\n",
    "\n",
    "def ndcg(gains, ideal_gains, k=5):\n",
    "    \"\"\"Computes NDCG given gains for a ranking as well as the ideal gains.\"\"\"\n",
    "    try:\n",
    "        return dcg(gains, k) / dcg(ideal_gains, k)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_type_path(type, type_hierarchy):\n",
    "    \"\"\"\n",
    "    Gets the type's path in the hierarchy (excluding the root type, like owl:Thing).\n",
    "    The path for each type is computed only once then cached in type_hierarchy,\n",
    "    to save computation.\n",
    "    \"\"\"\n",
    "    if not type in type_hierarchy:\n",
    "        type_hierarchy[type] = {'depth':1, 'parent':''}\n",
    "\n",
    "    if 'path' not in type_hierarchy[type]:\n",
    "        type_path = []\n",
    "        current_type = type\n",
    "        while current_type in type_hierarchy:\n",
    "            type_path.append(current_type)\n",
    "            current_type = type_hierarchy[current_type]['parent']\n",
    "        type_hierarchy[type]['path'] = type_path\n",
    "    return type_hierarchy[type]['path']\n",
    "\n",
    "def get_type_distance(type1, type2, type_hierarchy):\n",
    "    \"\"\"\n",
    "    Computes the distance between two types in the hierarchy.\n",
    "    Distance is defined to be the number of steps between them in the hierarchy,\n",
    "    if they lie on the same path (which is 0 if the two types match), and\n",
    "    infinity otherwise.\n",
    "    \"\"\"\n",
    "    type1_path = get_type_path(type1, type_hierarchy)\n",
    "    type2_path = get_type_path(type2, type_hierarchy)\n",
    "    distance = math.inf\n",
    "    if type1 in type2_path:\n",
    "        distance = type2_path.index(type1)\n",
    "    if type2 in type1_path:\n",
    "        distance = min(type1_path.index(type2), distance)\n",
    "    return distance\n",
    "\n",
    "def get_most_specific_types(types, type_hierarchy):\n",
    "    \"\"\"Filters a set of input types to most specific types w.r.t the type\n",
    "    hierarchy; i.e., super-types are removed.\"\"\"\n",
    "    filtered_types = set(types)\n",
    "    for type in types:\n",
    "        type_path = get_type_path(type, type_hierarchy)\n",
    "        for supertype in type_path[1:]:\n",
    "            if supertype in filtered_types:\n",
    "                filtered_types.remove(supertype)\n",
    "    return filtered_types\n",
    "\n",
    "def get_expanded_types(types, type_hierarchy):\n",
    "    \"\"\"Expands a set of types with both more specific and more generic types\n",
    "    (i.e., all super-types and sub-types).\"\"\"\n",
    "    expanded_types = set()\n",
    "    for type in types:\n",
    "        # Adding all supertypes.\n",
    "        expanded_types.update(get_type_path(type, type_hierarchy))\n",
    "        # Adding all subtypes (NOTE: this bit could be done more efficiently).\n",
    "        for type2 in type_hierarchy:\n",
    "            if type_hierarchy[type2]['depth'] <= type_hierarchy[type]['depth']:\n",
    "                continue\n",
    "            type2_path = get_type_path(type2, type_hierarchy)\n",
    "            if type in type2_path:\n",
    "                expanded_types.update(type2_path)\n",
    "    return expanded_types\n",
    "\n",
    "def compute_type_gains(predicted_types, gold_types, type_hierarchy, max_depth):\n",
    "    \"\"\"Computes gains for a ranked list of type predictions.\n",
    "\n",
    "    Following the definition of Linear gain in (Balog and Neumayer, CIKM'12),\n",
    "    the gain for a given predicted type is 0 if it is not on the same path with\n",
    "    any of the gold types, and otherwise it's $1-d(t,t_q)/h$ where $d(t,t_q)$ is\n",
    "    the distance between the predicted type and the closest matching gold type\n",
    "    in the type hierarchy and h is the maximum depth of the type hierarchy.\n",
    "\n",
    "    Args:\n",
    "        predicted_types: Ranked list of predicted types.\n",
    "        gold_types: List/set of gold types (i.e., perfect answers).\n",
    "        type_hierarchy: Dict with type hierarchy.\n",
    "        max_depth: Maximum depth of the type hierarchy.\n",
    "\n",
    "    Returns:\n",
    "        List with gain values corresponding to each item in predicted_types.\n",
    "    \"\"\"\n",
    "    gains = []\n",
    "    expanded_gold_types = get_expanded_types(gold_types, type_hierarchy)\n",
    "    for predicted_type in predicted_types:\n",
    "        if predicted_type in expanded_gold_types:\n",
    "            # Since not all gold types may lie on the same branch, we take the\n",
    "            # closest gold type for determining distance.\n",
    "            min_distance = math.inf\n",
    "            for gold_type in gold_types:\n",
    "                min_distance = min(get_type_distance(predicted_type, gold_type,\n",
    "                                                     type_hierarchy),\n",
    "                                   min_distance)\n",
    "            gains.append(1 - min_distance / max_depth)\n",
    "        else:\n",
    "            gains.append(0)\n",
    "    return gains\n",
    "\n",
    "def evaluate(result):\n",
    "\taccuracy, ndcg_5, ndcg_10 = [], [], []\n",
    "\tfor qId, obj in training_queries.items():\n",
    "\t\tif qId not in result:\n",
    "\t\t\tcontinue\n",
    "\t\t\n",
    "\t\tqTypes = obj['type'].split(' ')\n",
    "\t\tif len(qTypes) == 0:\n",
    "\t\t\tcontinue\n",
    "\t\t\t\n",
    "\t\tpredicted_category = result[qId].get('category', None)\n",
    "\t\tpredicted_type = result[qId].get('type', [None])\n",
    "\t\taccuracy.append(result[qId].get('match', 0))\n",
    "\n",
    "\t\t# Filters obj types to most specific ones in the hierarchy.\n",
    "\t\tobj_types = get_most_specific_types(qTypes, type_hierarchy)\n",
    "\t\tgains = compute_type_gains(predicted_type, obj_types, type_hierarchy, max_depth)\n",
    "\t\tideal_gains = sorted(\n",
    "\t\t\t\tcompute_type_gains(\n",
    "\t\t\t\tget_expanded_types(obj_types, type_hierarchy), obj_types,\n",
    "\t\t\t\ttype_hierarchy, max_depth), reverse=True)\n",
    "\n",
    "\t\tndcg_5.append(ndcg(gains, ideal_gains, k=5))\n",
    "\t\tndcg_10.append(ndcg(gains, ideal_gains, k=10))\n",
    "\n",
    "\tprint('Evaluation results:')\n",
    "\tprint('-------------------')\n",
    "\tprint('Category prediction (based on {} questions)'.format(len(accuracy)))\n",
    "\tprint('  Accuracy: {:5.3f}'.format(sum(accuracy) / len(accuracy)))\n",
    "\tprint('Type ranking (based on {} questions)'.format(len(ndcg_5)))\n",
    "\tprint('  NDCG@5:  {:5.3f}'.format(sum(ndcg_5) / len(ndcg_5)))\n",
    "\tprint('  NDCG@10: {:5.3f}'.format(sum(ndcg_10) / len(ndcg_10)))\n",
    "\n",
    "def write_result_to_file(res, file):\n",
    "    with open('./results/{}.csv'.format(file), 'w') as f:\n",
    "        for qId, obj in res.items():\n",
    "            f.write('{},{},{}\\n'.format(qId, obj['match'], ' '.join(obj['type'])))\n",
    "\n",
    "def read_result_from_file(file):\n",
    "    result = {}\n",
    "    with open('./results/{}.csv'.format(file), 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(',')\n",
    "            if len(line) != 3:\n",
    "                continue\n",
    "            result[line[0]] = {\n",
    "                'type': [v for v in line[-1].split(' ') if len(v) > 0],\n",
    "                'category': 'resource',\n",
    "                'match': int(line[1])\n",
    "            }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress - 50/9557 queries handled.\n",
      "Progress - 100/9557 queries handled.\n",
      "Progress - 150/9557 queries handled.\n",
      "Progress - 200/9557 queries handled.\n",
      "Progress - 250/9557 queries handled.\n",
      "Progress - 300/9557 queries handled.\n",
      "Progress - 350/9557 queries handled.\n",
      "Progress - 400/9557 queries handled.\n",
      "Progress - 450/9557 queries handled.\n",
      "Progress - 500/9557 queries handled.\n",
      "Progress - 550/9557 queries handled.\n",
      "Progress - 600/9557 queries handled.\n",
      "Progress - 650/9557 queries handled.\n",
      "Progress - 700/9557 queries handled.\n",
      "Progress - 750/9557 queries handled.\n",
      "Progress - 800/9557 queries handled.\n",
      "Progress - 850/9557 queries handled.\n",
      "Progress - 900/9557 queries handled.\n",
      "Progress - 950/9557 queries handled.\n",
      "Progress - 1000/9557 queries handled.\n"
     ]
    }
   ],
   "source": [
    "res_baseline = evaluate_baseline(es, amount=1000)\n",
    "write_result_to_file(res_baseline, 'baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results:\n",
      "-------------------\n",
      "Category prediction (based on 1000 questions)\n",
      "  Accuracy: 0.474\n",
      "Type ranking (based on 1000 questions)\n",
      "  NDCG@5:  0.309\n",
      "  NDCG@10: 0.395\n"
     ]
    }
   ],
   "source": [
    "res_baseline = read_result_from_file('baseline')\n",
    "evaluate(res_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(EMBEDDING_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress - 50/9557 queries handled.\n",
      "Progress - 100/9557 queries handled.\n",
      "Progress - 150/9557 queries handled.\n",
      "Progress - 200/9557 queries handled.\n",
      "Progress - 250/9557 queries handled.\n",
      "Progress - 300/9557 queries handled.\n",
      "Progress - 350/9557 queries handled.\n",
      "Progress - 400/9557 queries handled.\n",
      "Progress - 450/9557 queries handled.\n",
      "Progress - 500/9557 queries handled.\n",
      "Progress - 550/9557 queries handled.\n",
      "Progress - 600/9557 queries handled.\n",
      "Progress - 650/9557 queries handled.\n",
      "Progress - 700/9557 queries handled.\n",
      "Progress - 750/9557 queries handled.\n",
      "Progress - 800/9557 queries handled.\n",
      "Progress - 850/9557 queries handled.\n",
      "Progress - 900/9557 queries handled.\n",
      "Progress - 950/9557 queries handled.\n",
      "Progress - 1000/9557 queries handled.\n",
      "Evaluation results:\n",
      "-------------------\n",
      "Category prediction (based on 1000 questions)\n",
      "  Accuracy: 0.517\n",
      "Type ranking (based on 1000 questions)\n",
      "  NDCG@5:  0.397\n",
      "  NDCG@10: 0.501\n"
     ]
    }
   ],
   "source": [
    "res_adv = evaluate_advanced(es, model, k=1000, amount=1000)\n",
    "evaluate(res_adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different model dimensionality\n",
    "Run on 100queries, different dimensionality on model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress - 50/9557 queries handled.\n",
      "Progress - 100/9557 queries handled.\n",
      "Evaluation results:\n",
      "-------------------\n",
      "Category prediction (based on 100 questions)\n",
      "  Accuracy: 0.410\n",
      "Type ranking (based on 100 questions)\n",
      "  NDCG@5:  0.309\n",
      "  NDCG@10: 0.381\n"
     ]
    }
   ],
   "source": [
    "res_baseline = evaluate_baseline(es, amount=100)\n",
    "evaluate(res_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress - 50/9557 queries handled.\n",
      "Progress - 100/9557 queries handled.\n",
      "Evaluation results:\n",
      "-------------------\n",
      "Category prediction (based on 100 questions)\n",
      "  Accuracy: 0.470\n",
      "Type ranking (based on 100 questions)\n",
      "  NDCG@5:  0.331\n",
      "  NDCG@10: 0.430\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = KeyedVectors.load_word2vec_format(datapath(os.getcwd()+'/datasets/gensim/gensim.6B.50d.txt'))\n",
    "res_adv = evaluate_advanced(es, model, k=100, amount=100)\n",
    "evaluate(res_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress - 50/9557 queries handled.\n",
      "Progress - 100/9557 queries handled.\n",
      "Evaluation results:\n",
      "-------------------\n",
      "Category prediction (based on 100 questions)\n",
      "  Accuracy: 0.470\n",
      "Type ranking (based on 100 questions)\n",
      "  NDCG@5:  0.341\n",
      "  NDCG@10: 0.441\n"
     ]
    }
   ],
   "source": [
    "model = KeyedVectors.load_word2vec_format(datapath(os.getcwd()+'/datasets/gensim/gensim.6B.100d.txt'))\n",
    "res_adv = evaluate_advanced(es, model, k=100, amount=100)\n",
    "evaluate(res_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress - 50/9557 queries handled.\n",
      "Progress - 100/9557 queries handled.\n",
      "Evaluation results:\n",
      "-------------------\n",
      "Category prediction (based on 100 questions)\n",
      "  Accuracy: 0.460\n",
      "Type ranking (based on 100 questions)\n",
      "  NDCG@5:  0.325\n",
      "  NDCG@10: 0.428\n"
     ]
    }
   ],
   "source": [
    "model = KeyedVectors.load_word2vec_format(datapath(os.getcwd()+'/datasets/gensim/gensim.6B.200d.txt'))\n",
    "res_adv = evaluate_advanced(es, model, k=100, amount=100)\n",
    "evaluate(res_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress - 50/9557 queries handled.\n",
      "Progress - 100/9557 queries handled.\n",
      "Evaluation results:\n",
      "-------------------\n",
      "Category prediction (based on 100 questions)\n",
      "  Accuracy: 0.480\n",
      "Type ranking (based on 100 questions)\n",
      "  NDCG@5:  0.315\n",
      "  NDCG@10: 0.428\n"
     ]
    }
   ],
   "source": [
    "model = KeyedVectors.load_word2vec_format(datapath(os.getcwd()+'/datasets/gensim/gensim.6B.300d.txt'))\n",
    "res_adv = evaluate_advanced(es, model, k=100, amount=100)\n",
    "evaluate(res_adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different amount of collected documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress - 50/9557 queries handled.\n",
      "Progress - 100/9557 queries handled.\n",
      "Evaluation results:\n",
      "-------------------\n",
      "Category prediction (based on 100 questions)\n",
      "  Accuracy: 0.410\n",
      "Type ranking (based on 100 questions)\n",
      "  NDCG@5:  0.309\n",
      "  NDCG@10: 0.381\n"
     ]
    }
   ],
   "source": [
    "model = KeyedVectors.load_word2vec_format(datapath(os.getcwd()+'/datasets/gensim/gensim.6B.100d.txt'))\n",
    "\n",
    "res_baseline = evaluate_baseline(es, amount=100)\n",
    "evaluate(res_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress - 50/9557 queries handled.\n",
      "Progress - 100/9557 queries handled.\n",
      "Evaluation results:\n",
      "-------------------\n",
      "Category prediction (based on 100 questions)\n",
      "  Accuracy: 0.190\n",
      "Type ranking (based on 100 questions)\n",
      "  NDCG@5:  0.114\n",
      "  NDCG@10: 0.100\n"
     ]
    }
   ],
   "source": [
    "res_adv = evaluate_advanced(es, model, k=1, amount=100)\n",
    "evaluate(res_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress - 50/9557 queries handled.\n",
      "Progress - 100/9557 queries handled.\n",
      "Evaluation results:\n",
      "-------------------\n",
      "Category prediction (based on 100 questions)\n",
      "  Accuracy: 0.320\n",
      "Type ranking (based on 100 questions)\n",
      "  NDCG@5:  0.304\n",
      "  NDCG@10: 0.260\n"
     ]
    }
   ],
   "source": [
    "res_adv = evaluate_advanced(es, model, k=5, amount=100)\n",
    "evaluate(res_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress - 50/9557 queries handled.\n",
      "Progress - 100/9557 queries handled.\n",
      "Evaluation results:\n",
      "-------------------\n",
      "Category prediction (based on 100 questions)\n",
      "  Accuracy: 0.410\n",
      "Type ranking (based on 100 questions)\n",
      "  NDCG@5:  0.285\n",
      "  NDCG@10: 0.371\n"
     ]
    }
   ],
   "source": [
    "res_adv = evaluate_advanced(es, model, k=10, amount=100)\n",
    "evaluate(res_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress - 50/9557 queries handled.\n",
      "Progress - 100/9557 queries handled.\n",
      "Evaluation results:\n",
      "-------------------\n",
      "Category prediction (based on 100 questions)\n",
      "  Accuracy: 0.440\n",
      "Type ranking (based on 100 questions)\n",
      "  NDCG@5:  0.335\n",
      "  NDCG@10: 0.430\n"
     ]
    }
   ],
   "source": [
    "res_adv = evaluate_advanced(es, model, k=25, amount=100)\n",
    "evaluate(res_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress - 50/9557 queries handled.\n",
      "Progress - 100/9557 queries handled.\n",
      "Evaluation results:\n",
      "-------------------\n",
      "Category prediction (based on 100 questions)\n",
      "  Accuracy: 0.450\n",
      "Type ranking (based on 100 questions)\n",
      "  NDCG@5:  0.340\n",
      "  NDCG@10: 0.444\n"
     ]
    }
   ],
   "source": [
    "res_adv = evaluate_advanced(es, model, k=50, amount=100)\n",
    "evaluate(res_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress - 50/9557 queries handled.\n",
      "Progress - 100/9557 queries handled.\n",
      "Evaluation results:\n",
      "-------------------\n",
      "Category prediction (based on 100 questions)\n",
      "  Accuracy: 0.470\n",
      "Type ranking (based on 100 questions)\n",
      "  NDCG@5:  0.332\n",
      "  NDCG@10: 0.430\n"
     ]
    }
   ],
   "source": [
    "res_adv = evaluate_advanced(es, model, k=100, amount=100)\n",
    "evaluate(res_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress - 50/9557 queries handled.\n",
      "Progress - 100/9557 queries handled.\n",
      "Evaluation results:\n",
      "-------------------\n",
      "Category prediction (based on 100 questions)\n",
      "  Accuracy: 0.440\n",
      "Type ranking (based on 100 questions)\n",
      "  NDCG@5:  0.327\n",
      "  NDCG@10: 0.420\n"
     ]
    }
   ],
   "source": [
    "res_adv = evaluate_advanced(es, model, k=250, amount=100)\n",
    "evaluate(res_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress - 50/9557 queries handled.\n",
      "Progress - 100/9557 queries handled.\n",
      "Evaluation results:\n",
      "-------------------\n",
      "Category prediction (based on 100 questions)\n",
      "  Accuracy: 0.440\n",
      "Type ranking (based on 100 questions)\n",
      "  NDCG@5:  0.336\n",
      "  NDCG@10: 0.437\n"
     ]
    }
   ],
   "source": [
    "res_adv = evaluate_advanced(es, model, k=500, amount=100)\n",
    "evaluate(res_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress - 50/9557 queries handled.\n",
      "Progress - 100/9557 queries handled.\n",
      "Evaluation results:\n",
      "-------------------\n",
      "Category prediction (based on 100 questions)\n",
      "  Accuracy: 0.420\n",
      "Type ranking (based on 100 questions)\n",
      "  NDCG@5:  0.343\n",
      "  NDCG@10: 0.456\n"
     ]
    }
   ],
   "source": [
    "res_adv = evaluate_advanced(es, model, k=1000, amount=100)\n",
    "evaluate(res_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress - 50/9557 queries handled.\n",
      "Progress - 100/9557 queries handled.\n",
      "Evaluation results:\n",
      "-------------------\n",
      "Category prediction (based on 100 questions)\n",
      "  Accuracy: 0.440\n",
      "Type ranking (based on 100 questions)\n",
      "  NDCG@5:  0.336\n",
      "  NDCG@10: 0.437\n"
     ]
    }
   ],
   "source": [
    "res_adv = evaluate_advanced(es, model, k=5000, amount=100)\n",
    "evaluate(res_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress - 50/9557 queries handled.\n",
      "Progress - 100/9557 queries handled.\n",
      "Evaluation results:\n",
      "-------------------\n",
      "Category prediction (based on 100 questions)\n",
      "  Accuracy: 0.480\n",
      "Type ranking (based on 100 questions)\n",
      "  NDCG@5:  0.327\n",
      "  NDCG@10: 0.438\n"
     ]
    }
   ],
   "source": [
    "res_adv = evaluate_advanced(es, model, k=10000, amount=100)\n",
    "evaluate(res_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress - 50/9557 queries handled.\n",
      "Progress - 100/9557 queries handled.\n",
      "Evaluation results:\n",
      "-------------------\n",
      "Category prediction (based on 100 questions)\n",
      "  Accuracy: 0.480\n",
      "Type ranking (based on 100 questions)\n",
      "  NDCG@5:  0.339\n",
      "  NDCG@10: 0.453\n"
     ]
    }
   ],
   "source": [
    "model = KeyedVectors.load_word2vec_format(datapath(os.getcwd()+'/datasets/gensim/gensim.6B.300d.txt'))\n",
    "res_adv = evaluate_advanced(es, model, k=10000, amount=100)\n",
    "evaluate(res_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
