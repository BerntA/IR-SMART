{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import elasticsearch\n",
    "import time\n",
    "from elasticsearch import Elasticsearch, helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'name': 'ULTIMECIA',\n",
       " 'cluster_name': 'elasticsearch',\n",
       " 'cluster_uuid': 'MHYEAbcOS_i6ybp0d4NE2A',\n",
       " 'version': {'number': '7.9.1',\n",
       "  'build_flavor': 'default',\n",
       "  'build_type': 'zip',\n",
       "  'build_hash': '083627f112ba94dffc1232e8b42b73492789ef91',\n",
       "  'build_date': '2020-09-01T21:22:21.964974Z',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '8.6.2',\n",
       "  'minimum_wire_compatibility_version': '6.8.0',\n",
       "  'minimum_index_compatibility_version': '6.0.0-beta1'},\n",
       " 'tagline': 'You Know, for Search'}"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "es = Elasticsearch()\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIELDS = ['abstract', 'subject', 'instance']\n",
    "INDEX_NAME = 'fasttest'\n",
    "INDEX_SETTINGS = {\n",
    "    'mappings': {\n",
    "            'properties': {\n",
    "                'abstract': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                },\n",
    "                'subject': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                },\n",
    "                'instance': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTheIndex():\n",
    "    if es.indices.exists(INDEX_NAME):\n",
    "        es.indices.delete(index=INDEX_NAME)    \n",
    "    es.indices.create(index=INDEX_NAME, body=INDEX_SETTINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#createTheIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peek(filename, size, enc='utf-8'):\n",
    "    \"\"\"\n",
    "    Print out the first X lines in the file.\n",
    "    \"\"\"\n",
    "    if size <= 0:\n",
    "        print(\"Size must be greater than zero!\")\n",
    "        return\n",
    "\n",
    "    with open(filename, encoding=enc) as f:\n",
    "        for i,line in enumerate(f):\n",
    "            if (size >= 0) and (i >= size):\n",
    "                break\n",
    "            if i == 0: # Skip top line.\n",
    "                continue\n",
    "            print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITIES_PROCESSED = None\n",
    "DEBUGGING = False # If true, only test (index) on a small subset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseAbstract(data, line):\n",
    "    \"\"\"Parse a line from long_abstract.\"\"\"\n",
    "    if (line is None) or (line[0] == '#'):\n",
    "        return\n",
    "    line = line.strip().replace('\"', '').replace('\\'', '').replace('@en .', '').replace('/>', '>').split(' ')\n",
    "    if len(line) < 3:\n",
    "        return # Invalid line.\n",
    "    entity = line[0][1:-1].split('/')[-1].replace('_', ' ')\n",
    "    # TODO, long abstracts might need some more preprocessing, like removing symbols except for ',.- etc.. ??\n",
    "    value = ' '.join(line[2:]).replace('\\\\', '')\n",
    "    data.append({\n",
    "                \"_id\": entity, \n",
    "                \"_source\": {'abstract': value, 'subject': '', 'instance': 'Thing'}\n",
    "    })\n",
    "    if DEBUGGING:\n",
    "        ENTITIES_PROCESSED.add(entity) # Testing\n",
    "\n",
    "def parseSubject(data, line):\n",
    "    \"\"\"Parse a line from categories.\"\"\"\n",
    "    if (line is None) or (line[0] == '#'):\n",
    "        return None, None\n",
    "    line = line.strip().replace('/>', '>').split(' ')\n",
    "    if len(line) < 3:\n",
    "        return None, None # Invalid line.\n",
    "    entity = line[0][1:-1].split('/')[-1].replace('_', ' ')\n",
    "    value = line[2][1:-1].split('/')[-1][len('Category:'):].replace('_', ' ')\n",
    "    if not entity in data:\n",
    "        data[entity] = {\n",
    "            \"_id\": entity, \n",
    "            \"_source\": {\"doc\": {'subject': value}},\n",
    "            \"_op_type\": \"update\"\n",
    "        }\n",
    "    else:\n",
    "        data[entity]['_source']['doc']['subject'] = data[entity]['_source']['doc']['subject'] + ', ' + value # Spaghetti?!\n",
    "    return entity[0].upper(), entity\n",
    "\n",
    "def parseType(data, line):\n",
    "    \"\"\"Parse a line from instances.\"\"\"\n",
    "    if (line is None) or (line[0] == '#'):\n",
    "        return\n",
    "    line = line.strip().replace('/>', '>').split(' ')\n",
    "    if len(line) < 3:\n",
    "        return # Invalid line.\n",
    "    entity = line[0][1:-1].split('/')[-1].replace('_', ' ')\n",
    "    value = line[2][1:-1].split('/')[-1].replace('owl#', '').replace('_', ' ')\n",
    "    data.append({\n",
    "                \"_id\": entity, \n",
    "                \"_source\": {\"doc\": {'instance': value}},\n",
    "                \"_op_type\": \"update\"\n",
    "    })\n",
    "    \n",
    "def getBulkData(data):\n",
    "    \"\"\"\n",
    "    To prevent issues when debugging,\n",
    "    we only bulk data which was indexed @ abstract.\n",
    "    \"\"\"\n",
    "    if DEBUGGING:\n",
    "        return [d for d in data if (d['_id'] in ENTITIES_PROCESSED)]\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexData(size=5000):\n",
    "    \"\"\"\n",
    "    Index the data, size = how many entities to parse at a time.\n",
    "    size should not be much bigger than 20000, due to bulk index size limitations @ elasticsearch!\n",
    "    \"\"\"\n",
    "    global ENTITIES_PROCESSED\n",
    "    ENTITIES_PROCESSED = set()\n",
    "    files = [\n",
    "        ('datasets/DBpedia/long_abstracts_en.ttl', 'utf-8'),\n",
    "        ('datasets/DBpedia/article_categories_en.ttl', 'utf-8'),\n",
    "        ('datasets/DBpedia/instance_types_en.ttl', 'utf-8')\n",
    "    ]\n",
    "    try:\n",
    "        files = [open(f, 'r', encoding=e) for f, e in files] # Datasets to index.\n",
    "        listAbstract, listSubject, listType = [], {}, []\n",
    "        abstractFile, categoriesFile, instancesFile = files[0], files[1], files[2]\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Process abstracts first! (bulk)\n",
    "        for i, line in enumerate(abstractFile):\n",
    "            if i == 0: # Skip top line.\n",
    "                continue\n",
    "            parseAbstract(listAbstract, line)\n",
    "            if (len(listAbstract) > size):\n",
    "                helpers.bulk(es, listAbstract, index=INDEX_NAME, raise_on_error=False, raise_on_exception=False)\n",
    "                listAbstract.clear()\n",
    "                if DEBUGGING: # Only consider a small subset during test.\n",
    "                    break\n",
    "                \n",
    "        if len(listAbstract): # Still have some remaining items? Bulk them now.\n",
    "            helpers.bulk(es, listAbstract, index=INDEX_NAME, raise_on_error=False, raise_on_exception=False)\n",
    "            listAbstract.clear()\n",
    "            \n",
    "        print(\"Indexed abstracts.\")\n",
    "        print(\"Time Elapsed: {:.4f} sec.\".format((time.time()-start_time)))\n",
    "        \n",
    "        lineB, lineC = next(categoriesFile, None), next(instancesFile, None) # Skip top lines!\n",
    "        test1, test2 = False, False\n",
    "        \n",
    "        while (lineB or lineC):\n",
    "            if DEBUGGING and test1 and test2: # Limit to a small subset during testing.\n",
    "                break\n",
    "\n",
    "            if lineB:\n",
    "                lineB = next(categoriesFile, None)\n",
    "                \n",
    "            if lineC:\n",
    "                lineC = next(instancesFile, None)\n",
    "                \n",
    "            parseType(listType, lineC)\n",
    "            currSubjectChar, ent = parseSubject(listSubject, lineB)\n",
    "            \n",
    "            # When we have at least 'size' subjects (entities)\n",
    "            # Continue to the first next char which differs from the previous entry \n",
    "            # Which triggered the underneath condition.\n",
    "            # Add further entries until the first letter in the ent. changes.\n",
    "            # Bulk the entries until that entry!\n",
    "            if (len(listSubject) > size):\n",
    "                lastSubjectChar = currSubjectChar # Find the next first letter of ent. which differs from this. Then bulk.\n",
    "                newValue = None\n",
    "                while True:\n",
    "                    if lineB:\n",
    "                        lineB = next(categoriesFile, None)                        \n",
    "                    if lineB is None:\n",
    "                        break\n",
    "                    currSubjectChar, ent = parseSubject(listSubject, lineB)\n",
    "                    if currSubjectChar and (currSubjectChar != lastSubjectChar):\n",
    "                        newValue = listSubject[ent] # This value belongs to the next 'group', save it for that group.\n",
    "                        del listSubject[ent]\n",
    "                        break\n",
    "                helpers.bulk(es, getBulkData(listSubject.values()), index=INDEX_NAME, raise_on_error=False, raise_on_exception=False)\n",
    "                listSubject.clear()\n",
    "                if newValue: # Add the newest value back again.\n",
    "                    listSubject[ent] = newValue\n",
    "                test2 = True\n",
    "\n",
    "            if (len(listType) > size):\n",
    "                helpers.bulk(es, getBulkData(listType), index=INDEX_NAME, raise_on_error=False, raise_on_exception=False)\n",
    "                listType.clear()\n",
    "                test1 = True\n",
    "\n",
    "        # If there are remaining elements left, be sure to bulk index them!\n",
    "        if len(listSubject):\n",
    "            helpers.bulk(es, getBulkData(listSubject.values()), index=INDEX_NAME, raise_on_error=False, raise_on_exception=False)\n",
    "\n",
    "        if len(listType):\n",
    "            helpers.bulk(es, getBulkData(listType), index=INDEX_NAME, raise_on_error=False, raise_on_exception=False)\n",
    "        \n",
    "        print(\"Finished indexing successfully!\")\n",
    "        print(\"Time Elapsed: {:.4f} sec.\".format((time.time()-start_time)))\n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "        print(traceback.format_exc())\n",
    "    finally:\n",
    "        for f in files:\n",
    "            f.close()\n",
    "        listAbstract.clear()\n",
    "        listSubject.clear()\n",
    "        listType.clear()\n",
    "        ENTITIES_PROCESSED.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Takes between 2 to 3 hours!\n",
    "# indexData(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<http://dbpedia.org/resource/Animalia_(book)> <http://dbpedia.org/ontology/abstract> \"Animalia is an illustrated children's book by Graeme Base. It was originally published in 1986, followed by a tenth anniversary edition in 1996, and a 25th anniversary edition in 2012. Over three million copies have been sold. A special numbered and signed anniversary edition was also published in 1996, with an embossed gold jacket.\"@en .\n<http://dbpedia.org/resource/Actrius> <http://dbpedia.org/ontology/abstract> \"Actresses (Catalan: Actrius) is a 1997 Catalan language Spanish drama film produced and directed by Ventura Pons and based on the award-winning stage play E.R. by Josep Maria Benet i Jornet. The film has no male actors, with all roles played by females. The film was produced in 1996.\"@en .\n"
     ]
    }
   ],
   "source": [
    "peek('datasets/DBpedia/long_abstracts_en.ttl', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<http://dbpedia.org/resource/A> <http://purl.org/dc/terms/subject> <http://dbpedia.org/resource/Category:ISO_basic_Latin_letters> .\n<http://dbpedia.org/resource/A> <http://purl.org/dc/terms/subject> <http://dbpedia.org/resource/Category:Vowel_letters> .\n"
     ]
    }
   ],
   "source": [
    "peek('datasets/DBpedia/article_categories_en.ttl', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<http://dbpedia.org/resource/Anarchism> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/2002/07/owl#Thing> .\n<http://dbpedia.org/resource/Achilles> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/2002/07/owl#Thing> .\n"
     ]
    }
   ],
   "source": [
    "peek('datasets/DBpedia/instance_types_en.ttl', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'took': 324,\n",
       " 'timed_out': False,\n",
       " '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0},\n",
       " 'hits': {'total': {'value': 10000, 'relation': 'gte'},\n",
       "  'max_score': 18.70188,\n",
       "  'hits': [{'_index': 'fasttest',\n",
       "    '_type': '_doc',\n",
       "    '_id': 'Killing Kennedy',\n",
       "    '_score': 18.70188,\n",
       "    '_source': {'abstract': 'Killing Kennedy: The End of Camelot is a 2012 non-fiction book by Bill OReilly and Martin Dugard about the 1963 assassination of U.S. President John Fitzgerald Kennedy. It is a follow-up to OReillys 2011 book Killing Lincoln. Killing Kennedy was released on October 2, 2012 through Henry Holt and Company.',\n",
       "     'subject': '',\n",
       "     'instance': 'Book'}},\n",
       "   {'_index': 'fasttest',\n",
       "    '_type': '_doc',\n",
       "    '_id': 'The Killing of a President',\n",
       "    '_score': 16.92986,\n",
       "    '_source': {'abstract': 'The Killing of a President : The Complete Photographic Record of the JFK Assassination, the Conspiracy, and the Cover-up is a book by Robert J. Groden which discusses the assassination of John F Kennedy and challenges the findings of the Warren Commission which concluded Kennedy was killed by a lone assassin. (1990, Viking Studio Books ISBN 9780670852673)',\n",
       "     'subject': '',\n",
       "     'instance': ''}},\n",
       "   {'_index': 'fasttest',\n",
       "    '_type': '_doc',\n",
       "    '_id': 'Lee Harvey Oswald',\n",
       "    '_score': 16.375845,\n",
       "    '_source': {'abstract': 'Lee Harvey Oswald (October 18, 1939 â€“ November 24, 1963) was an American sniper who assassinated President John F. Kennedy on November 22, 1963. According to five U.S. government investigations, Oswald shot and killed Kennedy as he traveled by motorcade through Dealey Plaza in the city of Dallas, Texas. Oswald was a former U.S. Marine who defected to the Soviet Union in October 1959. He lived in the Belarusian city of Minsk until June 1962, at which time he returned to the United States with Marina, his Russian wife, eventually settling in Dallas. Following Kennedys assassination, Oswald was initially arrested for the murder of police officer J. D. Tippit, who was killed on a Dallas street approximately 45 minutes after President Kennedy was shot. Oswald was later charged with the murder of Kennedy; he denied shooting anybody, saying that he was a patsy. Two days later, while being transferred from the city jail to the county jail, Oswald was shot and mortally wounded by Dallas nightclub owner Jack Ruby in full view of television cameras broadcasting live. In September, 1964, the Warren Commission concluded that Oswald acted alone in assassinating Kennedy by firing three shots from the Texas School Book Depository. This conclusion was supported by previous investigations carried out by the FBI, the Secret Service, and the Dallas Police Department. Despite forensic, ballistic, and eyewitness evidence supporting the lone gunman theory, public opinion polls taken over the years have shown that most Americans believe that Oswald did not act alone, but conspired with others to kill the president, and the assassination has spawned numerous conspiracy theories.',\n",
       "     'subject': '1939 births, 1963 deaths, Warren Easton High School alumni, 1963 murders in the United States, 20th-century American criminals, American assassins, American communists, American criminal snipers, American defectors to the Soviet Union, American expatriates in the Soviet Union, American Marxists, American murder victims, American spree killers, Assassins of Presidents of the United States, Burials in Texas, Criminals from Louisiana, Criminals from Texas, Deaths by firearm in Texas, Deaths in police custody in the United States, Filmed deaths, Murdered criminals, People associated with the assassination of John F. Kennedy, People from New Orleans, People murdered in Texas, People of the Civil Air Patrol, United States Marines, Subjects of iconic photographs',\n",
       "     'instance': 'Person'}}]}}"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "es.search(index=INDEX_NAME, body={'query': {'match': {'abstract': 'who killed kennedy?'}}}, _source=True, size=3)"
   ]
  },
  {
   "source": [
    "### Remove entities/resources/docs with instance == \"Thing\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of documents in index before deletion: 4663102\nNumber of documents in index after deletion: 4663102\nTotal execution time: 0.017001628875732422\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(\"Number of documents in index before deletion: {}\".format(es.count(index=INDEX_NAME)[\"count\"]))\n",
    "\n",
    "BATCH_SIZE = 10000\n",
    "i = 0\n",
    "bulk_deletes = []\n",
    "for result in helpers.scan(es, index=INDEX_NAME, query={\"query\":{\"match\": {\"instance\": \"Thing\"}}}):\n",
    "    if i == BATCH_SIZE:\n",
    "        helpers.bulk(es, bulk_deletes)\n",
    "        bulk_deletes = []\n",
    "        i = 0\n",
    "\n",
    "    result['_op_type'] = 'delete'\n",
    "    bulk_deletes.append(result)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "helpers.bulk(es, bulk_deletes)\n",
    "\n",
    "print(\"Number of documents in index after deletion: {}\".format(es.count(index=INDEX_NAME)[\"count\"]))\n",
    "print(\"Total execution time: {}\".format(time.time()-start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of documents in index: 4663102\nAmount of documents with instance = 'Thing': 0\nAmount of documents with instance = '': 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents in index: {}\".format(es.count(index=INDEX_NAME)[\"count\"]))\n",
    "es.search(index=INDEX_NAME, body={\"query\":{\"match\": {\"instance\": \"Thing\"}}}, size=1).get(\"hits\", {}).get(\"total\", {}).get(\"value\", 0)\n",
    "print(\"Amount of documents with instance = 'Thing': {}\".format(es.search(index=INDEX_NAME, body={\"query\":{\"match\": {\"instance\": \"Thing\"}}}, size=1).get(\"hits\", {}).get(\"total\", {}).get(\"value\", 0)))\n",
    "print(\"Amount of documents with instance = '': {}\".format(es.search(index=INDEX_NAME, body={\"query\":{\"match\": {\"instance\": \"\"}}}, size=1).get(\"hits\", {}).get(\"total\", {}).get(\"value\", 0)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}