{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import traceback\n",
    "import elasticsearch\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from elasticsearch import Elasticsearch, helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'BERNTA-PC',\n",
       " 'cluster_name': 'elasticsearch',\n",
       " 'cluster_uuid': 'IP06yo9vScKZA1ZTb8R9HA',\n",
       " 'version': {'number': '7.9.2',\n",
       "  'build_flavor': 'default',\n",
       "  'build_type': 'zip',\n",
       "  'build_hash': 'd34da0ea4a966c4e49417f2da2f244e3e97b4e6e',\n",
       "  'build_date': '2020-09-23T00:45:33.626720Z',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '8.6.2',\n",
       "  'minimum_wire_compatibility_version': '6.8.0',\n",
       "  'minimum_index_compatibility_version': '6.0.0-beta1'},\n",
       " 'tagline': 'You Know, for Search'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = Elasticsearch()\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIELDS = ['abstract', 'subject', 'instance']\n",
    "INDEX_NAME = 'fasttest'\n",
    "INDEX_SETTINGS = {\n",
    "    'mappings': {\n",
    "            'properties': {\n",
    "                'abstract': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                },\n",
    "                'subject': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                },\n",
    "                'instance': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'fasttest'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if es.indices.exists(INDEX_NAME):\n",
    "    es.indices.delete(index=INDEX_NAME)    \n",
    "es.indices.create(index=INDEX_NAME, body=INDEX_SETTINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words = set(stopwords.words('english'))\n",
    "#print(stop_words) # Needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peek(filename, size, enc='utf-8'):\n",
    "    \"\"\"\n",
    "    Print out the first X lines in the file.\n",
    "    \"\"\"\n",
    "    if size <= 0:\n",
    "        print(\"Size must be greater than zero!\")\n",
    "        return\n",
    "\n",
    "    with open(filename, encoding=enc) as f:\n",
    "        for i,line in enumerate(f):\n",
    "            if (size >= 0) and (i >= size):\n",
    "                break\n",
    "            if i == 0: # Skip top line.\n",
    "                continue\n",
    "            print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITIES_PROCESSED = None\n",
    "DEBUGGING = True # If true, only test (index) on a small subset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseAbstract(data, line):\n",
    "    \"\"\"Parse a line from long_abstract.\"\"\"\n",
    "    if line is None:\n",
    "        return\n",
    "    line = line.strip().replace('\"', '').replace('\\'', '').replace('@en .', '').replace('/>', '>').split(' ')\n",
    "    if len(line) < 3:\n",
    "        return # Invalid line.\n",
    "    entity = line[0][1:-1].split('/')[-1].replace('_', ' ')\n",
    "    # TODO, long abstracts might need some more preprocessing, like removing symbols except for ',.- etc.. ??\n",
    "    value = ' '.join(line[2:]).replace('\\\\', '')\n",
    "    data.append({\n",
    "                \"_id\": entity, \n",
    "                \"_source\": {'abstract': value, 'subject': '', 'instance': ''}\n",
    "    })\n",
    "    if DEBUGGING:\n",
    "        ENTITIES_PROCESSED.add(entity) # Testing\n",
    "\n",
    "def parseSubject(data, line):\n",
    "    \"\"\"Parse a line from categories.\"\"\"\n",
    "    if line is None:\n",
    "        return None, None\n",
    "    line = line.strip().replace('/>', '>').split(' ')\n",
    "    if len(line) < 3:\n",
    "        return None, None # Invalid line.\n",
    "    entity = line[0][1:-1].split('/')[-1].replace('_', ' ')\n",
    "    value = line[2][1:-1].split('/')[-1][len('Category:'):].replace('_', ' ')\n",
    "    if not entity in data:\n",
    "        data[entity] = {\n",
    "            \"_id\": entity, \n",
    "            \"_source\": {\"doc\": {'subject': value}},\n",
    "            \"_op_type\": \"update\"\n",
    "        }\n",
    "    else:\n",
    "        data[entity]['_source']['doc']['subject'] = data[entity]['_source']['doc']['subject'] + ', ' + value # Spaghetti?!\n",
    "                \n",
    "    return entity[0].upper(), entity\n",
    "\n",
    "def parseType(data, line):\n",
    "    \"\"\"Parse a line from instances.\"\"\"\n",
    "    if line is None:\n",
    "        return\n",
    "    line = line.strip().replace('/>', '>').split(' ')\n",
    "    if len(line) < 3:\n",
    "        return # Invalid line.\n",
    "    entity = line[0][1:-1].split('/')[-1].replace('__', ' ').replace('_', ' ')\n",
    "    value = line[2][1:-1].split('/')[-1].replace('owl#', '').replace('__', ' ').replace('_', ' ')\n",
    "    data.append({\n",
    "                \"_id\": entity, \n",
    "                \"_source\": {\"doc\": {'instance': value}},\n",
    "                \"_op_type\": \"update\"\n",
    "    })\n",
    "    \n",
    "def getBulkData(data):\n",
    "    \"\"\"\n",
    "    To prevent issues when debugging,\n",
    "    we only bulk data which was indexed @ abstract.\n",
    "    \"\"\"\n",
    "    if DEBUGGING:\n",
    "        return [d for d in data if (d['_id'] in ENTITIES_PROCESSED)]\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "def indexData(size=5000):\n",
    "    \"\"\"\n",
    "    Index the data, size = how many entities to parse at a time.\n",
    "    size should not be much bigger than 20000, due to bulk index size limitations @ elasticsearch!\n",
    "    \"\"\"\n",
    "    global ENTITIES_PROCESSED\n",
    "    ENTITIES_PROCESSED = set()\n",
    "    files = [\n",
    "        ('datasets/DBpedia/long_abstracts_en.ttl', 'utf-8'),\n",
    "        ('datasets/DBpedia/article_categories_en.ttl', 'utf-8'),\n",
    "        ('datasets/DBpedia/instance_types_en.ttl', 'utf-8')\n",
    "    ]\n",
    "    try:\n",
    "        files = [open(f, 'r', encoding=e) for f, e in files] # Datasets to index.\n",
    "        listAbstract, listSubject, listType = [], {}, []\n",
    "        abstractFile, categoriesFile, instancesFile = files[0], files[1], files[2]\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Process abstracts first! (bulk)\n",
    "        for i, line in enumerate(abstractFile):\n",
    "            if i == 0: # Skip top line.\n",
    "                continue\n",
    "            parseAbstract(listAbstract, line)\n",
    "            if (len(listAbstract) > size):\n",
    "                helpers.bulk(es, listAbstract, index=INDEX_NAME)\n",
    "                listAbstract.clear()\n",
    "                if DEBUGGING: # Only consider a small subset during test.\n",
    "                    break\n",
    "                \n",
    "        if len(listAbstract): # Still have some remaining items? Bulk them now.\n",
    "            helpers.bulk(es, listAbstract, index=INDEX_NAME)\n",
    "            listAbstract.clear()\n",
    "            \n",
    "        print(\"Indexed abstracts.\")\n",
    "        print(\"Time Elapsed: {:.4f} sec.\".format((time.time()-start_time)))\n",
    "        \n",
    "        lineB, lineC = next(categoriesFile), next(instancesFile) # Skip top lines!\n",
    "        test1, test2 = False, False\n",
    "        \n",
    "        while (lineB or lineC):\n",
    "            if DEBUGGING and test1 and test2: # Limit to a small subset during testing.\n",
    "                break\n",
    "\n",
    "            if lineB:\n",
    "                lineB = next(categoriesFile)\n",
    "                \n",
    "            if lineC:\n",
    "                lineC = next(instancesFile)\n",
    "                \n",
    "            parseType(listType, lineC)\n",
    "            currSubjectChar, ent = parseSubject(listSubject, lineB)\n",
    "            \n",
    "            # When we have at least 'size' subjects (entities)\n",
    "            # Continue to the first next char which differs from the previous entry \n",
    "            # Which triggered the underneath condition.\n",
    "            # Add further entries until the first letter in the ent. changes.\n",
    "            # Bulk the entries until that entry!\n",
    "            if (len(listSubject) > size):\n",
    "                lastSubjectChar = currSubjectChar # Find the next first letter of ent. which differs from this. Then bulk.\n",
    "                newValue = None\n",
    "                while True:\n",
    "                    if lineB:\n",
    "                        lineB = next(categoriesFile)                        \n",
    "                    if lineB is None:\n",
    "                        break\n",
    "                    currSubjectChar, ent = parseSubject(listSubject, lineB)\n",
    "                    if currSubjectChar and (currSubjectChar != lastSubjectChar):\n",
    "                        newValue = listSubject[ent] # This value belongs to the next 'group', save it for that group.\n",
    "                        del listSubject[ent]\n",
    "                        break\n",
    "                helpers.bulk(es, getBulkData(listSubject.values()), index=INDEX_NAME)\n",
    "                listSubject.clear()\n",
    "                if newValue: # Add the newest value back again.\n",
    "                    listSubject[ent] = newValue\n",
    "                test2 = True\n",
    "\n",
    "            if (len(listType) > size):\n",
    "                helpers.bulk(es, getBulkData(listType), index=INDEX_NAME)\n",
    "                listType.clear()\n",
    "                test1 = True\n",
    "\n",
    "        # If there are remaining elements left, be sure to bulk index them!\n",
    "        if len(listSubject):\n",
    "            helpers.bulk(es, getBulkData(listSubject.values()), index=INDEX_NAME)\n",
    "\n",
    "        if len(listType):\n",
    "            helpers.bulk(es, getBulkData(listType), index=INDEX_NAME)\n",
    "        \n",
    "        print(\"Finished indexing successfully!\")\n",
    "        print(\"Time Elapsed: {:.4f} sec.\".format((time.time()-start_time)))\n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "        print(traceback.format_exc())\n",
    "    finally:\n",
    "        for f in files:\n",
    "            f.close()\n",
    "        listAbstract.clear()\n",
    "        listSubject.clear()\n",
    "        listType.clear()\n",
    "        ENTITIES_PROCESSED.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed abstracts.\n",
      "Time Elapsed: 6.7424 sec.\n",
      "Finished indexing successfully!\n",
      "Time Elapsed: 18.0730 sec.\n"
     ]
    }
   ],
   "source": [
    "indexData(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<http://dbpedia.org/resource/Animalia_(book)> <http://dbpedia.org/ontology/abstract> \"Animalia is an illustrated children's book by Graeme Base. It was originally published in 1986, followed by a tenth anniversary edition in 1996, and a 25th anniversary edition in 2012. Over three million copies have been sold. A special numbered and signed anniversary edition was also published in 1996, with an embossed gold jacket.\"@en .\n",
      "<http://dbpedia.org/resource/Actrius> <http://dbpedia.org/ontology/abstract> \"Actresses (Catalan: Actrius) is a 1997 Catalan language Spanish drama film produced and directed by Ventura Pons and based on the award-winning stage play E.R. by Josep Maria Benet i Jornet. The film has no male actors, with all roles played by females. The film was produced in 1996.\"@en .\n"
     ]
    }
   ],
   "source": [
    "peek('datasets/DBpedia/long_abstracts_en.ttl', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<http://dbpedia.org/resource/A> <http://purl.org/dc/terms/subject> <http://dbpedia.org/resource/Category:ISO_basic_Latin_letters> .\n",
      "<http://dbpedia.org/resource/A> <http://purl.org/dc/terms/subject> <http://dbpedia.org/resource/Category:Vowel_letters> .\n"
     ]
    }
   ],
   "source": [
    "peek('datasets/DBpedia/article_categories_en.ttl', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<http://dbpedia.org/resource/Anarchism> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/2002/07/owl#Thing> .\n",
      "<http://dbpedia.org/resource/Achilles> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/2002/07/owl#Thing> .\n"
     ]
    }
   ],
   "source": [
    "peek('datasets/DBpedia/instance_types_en.ttl', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<http://dbpedia.org/resource/Autism> <http://dbpedia.org/ontology/icd10> \"F84.0\" .\n",
      "<http://dbpedia.org/resource/Autism> <http://dbpedia.org/ontology/icd9> \"299.00\" .\n"
     ]
    }
   ],
   "source": [
    "peek('datasets/DBpedia/mappingbased_literals_en.ttl', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<http://dbpedia.org/resource/Anarchism> <http://www.w3.org/2000/01/rdf-schema#seeAlso> <http://dbpedia.org/resource/Anarchist_terminology> .\n",
      "<http://dbpedia.org/resource/Anarchism> <http://www.w3.org/2000/01/rdf-schema#seeAlso> <http://dbpedia.org/resource/Anarchism> .\n"
     ]
    }
   ],
   "source": [
    "peek('datasets/DBpedia/mappingbased_objects_en.ttl', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'took': 3,\n",
       " 'timed_out': False,\n",
       " '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0},\n",
       " 'hits': {'total': {'value': 85, 'relation': 'eq'},\n",
       "  'max_score': 7.1047535,\n",
       "  'hits': [{'_index': 'fasttest',\n",
       "    '_type': '_doc',\n",
       "    '_id': 'Aeneas',\n",
       "    '_score': 7.1047535,\n",
       "    '_source': {'abstract': 'In Greco-Roman mythology, Aeneas (/ᵻˈniːəs/; Greek: Αἰνείας, Aineías, possibly derived from Greek αἰνή meaning praised) was a Trojan hero, the son of the prince Anchises and the goddess Venus (Aphrodite). His father was a first cousin of King Priam of Troy (both being grandsons of Ilus, founder of Troy), making Aeneas a second cousin to Priams children (such as Hector and Paris). He is a character in Greek mythology and is mentioned in Homers Iliad. Aeneas receives full treatment in Roman mythology, most extensively in Virgils Aeneid where he is an ancestor of Romulus and Remus. He became the first true hero of Rome.',\n",
       "     'subject': 'Trojans, Characters in the Aeneid, Characters in the Iliad, Characters in works by Geoffrey of Monmouth, Heroes in mythology and legend, Offspring of Aphrodite, People of the Trojan War, Roman mythology, Heroes who ventured to Hades',\n",
       "     'instance': 'Thing'}},\n",
       "   {'_index': 'fasttest',\n",
       "    '_type': '_doc',\n",
       "    '_id': 'Baldr',\n",
       "    '_score': 6.941183,\n",
       "    '_source': {'abstract': 'Baldr (also Balder, Baldur) is a god in Norse mythology, who is given a central role in the mythology. Despite this his precise function is rather disputed. He is often interpreted as the god of love, peace, forgiveness, justice, light or purity, but was not directly attested as a god of such. He is the second son of Odin and the goddess Frigg. His twin brother is the blind god Höðr.',\n",
       "     'subject': 'Germanic deities, Life-death-rebirth gods, Solar gods, Æsir, Sons of Odin',\n",
       "     'instance': 'Thing'}},\n",
       "   {'_index': 'fasttest',\n",
       "    '_type': '_doc',\n",
       "    '_id': 'Banshee',\n",
       "    '_score': 6.799015,\n",
       "    '_source': {'abstract': 'A banshee (/ˈbænʃiː/ BAN-shee, Modern Irish bean sí, from Old Irish: ban síde, pronounced [bʲan ˈʃiːðʲe], woman of the fairy mound or fairy woman) is a female spirit in Irish mythology who heralds the death of a family member, usually by shrieking or keening. Her name is connected to the mythologically-important tumuli or mounds that dot the Irish countryside, which are known as síde (singular síd) in Old Irish.',\n",
       "     'subject': 'Fairies, European folklore, Irish folklore, Irish legendary creatures, Scottish mythology, Irish ghosts, Female legendary creatures, Supernatural legends, Celtic folklore',\n",
       "     'instance': ''}},\n",
       "   {'_index': 'fasttest',\n",
       "    '_type': '_doc',\n",
       "    '_id': 'Bragi',\n",
       "    '_score': 6.6889896,\n",
       "    '_source': {'abstract': 'Bragi is the skaldic god of poetry in Norse mythology.',\n",
       "     'subject': 'Æsir, Knowledge gods',\n",
       "     'instance': ''}},\n",
       "   {'_index': 'fasttest',\n",
       "    '_type': '_doc',\n",
       "    '_id': 'Christian mythology',\n",
       "    '_score': 6.6889896,\n",
       "    '_source': {'abstract': 'Christian mythology is the body of myths associated with Christianity.',\n",
       "     'subject': 'Christian mythology, Abrahamic mythology, Jewish mythology',\n",
       "     'instance': 'Thing'}}]}}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.search(index=INDEX_NAME, body={'query': {'match': {'abstract': 'mythology'}}}, _source=True, size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4931948 abstracts\n",
    "# 850298+ cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
