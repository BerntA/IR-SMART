{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import elasticsearch\n",
    "import time\n",
    "from elasticsearch import Elasticsearch, helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'BERNTA-PC',\n",
       " 'cluster_name': 'elasticsearch',\n",
       " 'cluster_uuid': 'IP06yo9vScKZA1ZTb8R9HA',\n",
       " 'version': {'number': '7.9.2',\n",
       "  'build_flavor': 'default',\n",
       "  'build_type': 'zip',\n",
       "  'build_hash': 'd34da0ea4a966c4e49417f2da2f244e3e97b4e6e',\n",
       "  'build_date': '2020-09-23T00:45:33.626720Z',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '8.6.2',\n",
       "  'minimum_wire_compatibility_version': '6.8.0',\n",
       "  'minimum_index_compatibility_version': '6.0.0-beta1'},\n",
       " 'tagline': 'You Know, for Search'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = Elasticsearch()\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIELDS = ['abstract', 'subject', 'instance']\n",
    "INDEX_NAME = 'fasttest'\n",
    "INDEX_SETTINGS = {\n",
    "    'mappings': {\n",
    "            'properties': {\n",
    "                'abstract': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                },\n",
    "                'subject': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                },\n",
    "                'instance': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTheIndex():\n",
    "    if es.indices.exists(INDEX_NAME):\n",
    "        es.indices.delete(index=INDEX_NAME)    \n",
    "    es.indices.create(index=INDEX_NAME, body=INDEX_SETTINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#createTheIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peek(filename, size, enc='utf-8'):\n",
    "    \"\"\"\n",
    "    Print out the first X lines in the file.\n",
    "    \"\"\"\n",
    "    if size <= 0:\n",
    "        print(\"Size must be greater than zero!\")\n",
    "        return\n",
    "\n",
    "    with open(filename, encoding=enc) as f:\n",
    "        for i,line in enumerate(f):\n",
    "            if (size >= 0) and (i >= size):\n",
    "                break\n",
    "            if i == 0: # Skip top line.\n",
    "                continue\n",
    "            print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITIES_PROCESSED = None\n",
    "DEBUGGING = False # If true, only test (index) on a small subset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseAbstract(data, line):\n",
    "    \"\"\"Parse a line from long_abstract.\"\"\"\n",
    "    if (line is None) or (line[0] == '#'):\n",
    "        return\n",
    "    line = line.strip().replace('\"', '').replace('\\'', '').replace('@en .', '').replace('/>', '>').split(' ')\n",
    "    if len(line) < 3:\n",
    "        return # Invalid line.\n",
    "    entity = line[0][1:-1].split('/')[-1].replace('_', ' ')\n",
    "    # TODO, long abstracts might need some more preprocessing, like removing symbols except for ',.- etc.. ??\n",
    "    value = ' '.join(line[2:]).replace('\\\\', '')\n",
    "    data.append({\n",
    "                \"_id\": entity, \n",
    "                \"_source\": {'abstract': value, 'subject': '', 'instance': 'Thing'}\n",
    "    })\n",
    "    if DEBUGGING:\n",
    "        ENTITIES_PROCESSED.add(entity) # Testing\n",
    "\n",
    "def parseSubject(data, line):\n",
    "    \"\"\"Parse a line from categories.\"\"\"\n",
    "    if (line is None) or (line[0] == '#'):\n",
    "        return None, None\n",
    "    line = line.strip().replace('/>', '>').split(' ')\n",
    "    if len(line) < 3:\n",
    "        return None, None # Invalid line.\n",
    "    entity = line[0][1:-1].split('/')[-1].replace('_', ' ')\n",
    "    value = line[2][1:-1].split('/')[-1][len('Category:'):].replace('_', ' ')\n",
    "    if not entity in data:\n",
    "        data[entity] = {\n",
    "            \"_id\": entity, \n",
    "            \"_source\": {\"doc\": {'subject': value}},\n",
    "            \"_op_type\": \"update\"\n",
    "        }\n",
    "    else:\n",
    "        data[entity]['_source']['doc']['subject'] = data[entity]['_source']['doc']['subject'] + ', ' + value # Spaghetti?!\n",
    "    return entity[0].upper(), entity\n",
    "\n",
    "def parseType(data, line):\n",
    "    \"\"\"Parse a line from instances.\"\"\"\n",
    "    if (line is None) or (line[0] == '#'):\n",
    "        return\n",
    "    line = line.strip().replace('/>', '>').split(' ')\n",
    "    if len(line) < 3:\n",
    "        return # Invalid line.\n",
    "    entity = line[0][1:-1].split('/')[-1].replace('_', ' ')\n",
    "    value = line[2][1:-1].split('/')[-1].replace('owl#', '').replace('_', ' ')\n",
    "    data.append({\n",
    "                \"_id\": entity, \n",
    "                \"_source\": {\"doc\": {'instance': value}},\n",
    "                \"_op_type\": \"update\"\n",
    "    })\n",
    "    \n",
    "def getBulkData(data):\n",
    "    \"\"\"\n",
    "    To prevent issues when debugging,\n",
    "    we only bulk data which was indexed @ abstract.\n",
    "    \"\"\"\n",
    "    if DEBUGGING:\n",
    "        return [d for d in data if (d['_id'] in ENTITIES_PROCESSED)]\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexData(size=5000):\n",
    "    \"\"\"\n",
    "    Index the data, size = how many entities to parse at a time.\n",
    "    size should not be much bigger than 20000, due to bulk index size limitations @ elasticsearch!\n",
    "    \"\"\"\n",
    "    global ENTITIES_PROCESSED\n",
    "    ENTITIES_PROCESSED = set()\n",
    "    files = [\n",
    "        ('datasets/DBpedia/long_abstracts_en.ttl', 'utf-8'),\n",
    "        ('datasets/DBpedia/article_categories_en.ttl', 'utf-8'),\n",
    "        ('datasets/DBpedia/instance_types_en.ttl', 'utf-8')\n",
    "    ]\n",
    "    try:\n",
    "        files = [open(f, 'r', encoding=e) for f, e in files] # Datasets to index.\n",
    "        listAbstract, listSubject, listType = [], {}, []\n",
    "        abstractFile, categoriesFile, instancesFile = files[0], files[1], files[2]\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Process abstracts first! (bulk)\n",
    "        for i, line in enumerate(abstractFile):\n",
    "            if i == 0: # Skip top line.\n",
    "                continue\n",
    "            parseAbstract(listAbstract, line)\n",
    "            if (len(listAbstract) > size):\n",
    "                helpers.bulk(es, listAbstract, index=INDEX_NAME, raise_on_error=False, raise_on_exception=False)\n",
    "                listAbstract.clear()\n",
    "                if DEBUGGING: # Only consider a small subset during test.\n",
    "                    break\n",
    "                \n",
    "        if len(listAbstract): # Still have some remaining items? Bulk them now.\n",
    "            helpers.bulk(es, listAbstract, index=INDEX_NAME, raise_on_error=False, raise_on_exception=False)\n",
    "            listAbstract.clear()\n",
    "            \n",
    "        print(\"Indexed abstracts.\")\n",
    "        print(\"Time Elapsed: {:.4f} sec.\".format((time.time()-start_time)))\n",
    "        \n",
    "        lineB, lineC = next(categoriesFile, None), next(instancesFile, None) # Skip top lines!\n",
    "        test1, test2 = False, False\n",
    "        \n",
    "        while (lineB or lineC):\n",
    "            if DEBUGGING and test1 and test2: # Limit to a small subset during testing.\n",
    "                break\n",
    "\n",
    "            if lineB:\n",
    "                lineB = next(categoriesFile, None)\n",
    "                \n",
    "            if lineC:\n",
    "                lineC = next(instancesFile, None)\n",
    "                \n",
    "            parseType(listType, lineC)\n",
    "            currSubjectChar, ent = parseSubject(listSubject, lineB)\n",
    "            \n",
    "            # When we have at least 'size' subjects (entities)\n",
    "            # Continue to the first next char which differs from the previous entry \n",
    "            # Which triggered the underneath condition.\n",
    "            # Add further entries until the first letter in the ent. changes.\n",
    "            # Bulk the entries until that entry!\n",
    "            if (len(listSubject) > size):\n",
    "                lastSubjectChar = currSubjectChar # Find the next first letter of ent. which differs from this. Then bulk.\n",
    "                newValue = None\n",
    "                while True:\n",
    "                    if lineB:\n",
    "                        lineB = next(categoriesFile, None)                        \n",
    "                    if lineB is None:\n",
    "                        break\n",
    "                    currSubjectChar, ent = parseSubject(listSubject, lineB)\n",
    "                    if currSubjectChar and (currSubjectChar != lastSubjectChar):\n",
    "                        newValue = listSubject[ent] # This value belongs to the next 'group', save it for that group.\n",
    "                        del listSubject[ent]\n",
    "                        break\n",
    "                helpers.bulk(es, getBulkData(listSubject.values()), index=INDEX_NAME, raise_on_error=False, raise_on_exception=False)\n",
    "                listSubject.clear()\n",
    "                if newValue: # Add the newest value back again.\n",
    "                    listSubject[ent] = newValue\n",
    "                test2 = True\n",
    "\n",
    "            if (len(listType) > size):\n",
    "                helpers.bulk(es, getBulkData(listType), index=INDEX_NAME, raise_on_error=False, raise_on_exception=False)\n",
    "                listType.clear()\n",
    "                test1 = True\n",
    "\n",
    "        # If there are remaining elements left, be sure to bulk index them!\n",
    "        if len(listSubject):\n",
    "            helpers.bulk(es, getBulkData(listSubject.values()), index=INDEX_NAME, raise_on_error=False, raise_on_exception=False)\n",
    "\n",
    "        if len(listType):\n",
    "            helpers.bulk(es, getBulkData(listType), index=INDEX_NAME, raise_on_error=False, raise_on_exception=False)\n",
    "        \n",
    "        print(\"Finished indexing successfully!\")\n",
    "        print(\"Time Elapsed: {:.4f} sec.\".format((time.time()-start_time)))\n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "        print(traceback.format_exc())\n",
    "    finally:\n",
    "        for f in files:\n",
    "            f.close()\n",
    "        listAbstract.clear()\n",
    "        listSubject.clear()\n",
    "        listType.clear()\n",
    "        ENTITIES_PROCESSED.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Takes between 2 to 3 hours!\n",
    "# indexData(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<http://dbpedia.org/resource/Animalia_(book)> <http://dbpedia.org/ontology/abstract> \"Animalia is an illustrated children's book by Graeme Base. It was originally published in 1986, followed by a tenth anniversary edition in 1996, and a 25th anniversary edition in 2012. Over three million copies have been sold. A special numbered and signed anniversary edition was also published in 1996, with an embossed gold jacket.\"@en .\n",
      "<http://dbpedia.org/resource/Actrius> <http://dbpedia.org/ontology/abstract> \"Actresses (Catalan: Actrius) is a 1997 Catalan language Spanish drama film produced and directed by Ventura Pons and based on the award-winning stage play E.R. by Josep Maria Benet i Jornet. The film has no male actors, with all roles played by females. The film was produced in 1996.\"@en .\n"
     ]
    }
   ],
   "source": [
    "peek('datasets/DBpedia/long_abstracts_en.ttl', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<http://dbpedia.org/resource/A> <http://purl.org/dc/terms/subject> <http://dbpedia.org/resource/Category:ISO_basic_Latin_letters> .\n",
      "<http://dbpedia.org/resource/A> <http://purl.org/dc/terms/subject> <http://dbpedia.org/resource/Category:Vowel_letters> .\n"
     ]
    }
   ],
   "source": [
    "peek('datasets/DBpedia/article_categories_en.ttl', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<http://dbpedia.org/resource/Anarchism> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/2002/07/owl#Thing> .\n",
      "<http://dbpedia.org/resource/Achilles> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/2002/07/owl#Thing> .\n"
     ]
    }
   ],
   "source": [
    "peek('datasets/DBpedia/instance_types_en.ttl', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove entities/resources/docs with instance == \"Thing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in index before deletion: 4663102\n",
      "Number of documents in index after deletion: 4663102\n",
      "Total execution time: 0.017001628875732422\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(\"Number of documents in index before deletion: {}\".format(es.count(index=INDEX_NAME)[\"count\"]))\n",
    "\n",
    "BATCH_SIZE = 10000\n",
    "i = 0\n",
    "bulk_deletes = []\n",
    "for result in helpers.scan(es, index=INDEX_NAME, query={\"query\":{\"match\": {\"instance\": \"Thing\"}}}):\n",
    "    if i == BATCH_SIZE:\n",
    "        helpers.bulk(es, bulk_deletes)\n",
    "        bulk_deletes = []\n",
    "        i = 0\n",
    "\n",
    "    result['_op_type'] = 'delete'\n",
    "    bulk_deletes.append(result)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "helpers.bulk(es, bulk_deletes)\n",
    "\n",
    "print(\"Number of documents in index after deletion: {}\".format(es.count(index=INDEX_NAME)[\"count\"]))\n",
    "print(\"Total execution time: {}\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in index: 4663102\n",
      "Amount of documents with instance = 'Thing': 0\n",
      "Amount of documents with instance = '': 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents in index: {}\".format(es.count(index=INDEX_NAME)[\"count\"]))\n",
    "es.search(index=INDEX_NAME, body={\"query\":{\"match\": {\"instance\": \"Thing\"}}}, size=1).get(\"hits\", {}).get(\"total\", {}).get(\"value\", 0)\n",
    "print(\"Amount of documents with instance = 'Thing': {}\".format(es.search(index=INDEX_NAME, body={\"query\":{\"match\": {\"instance\": \"Thing\"}}}, size=1).get(\"hits\", {}).get(\"total\", {}).get(\"value\", 0)))\n",
    "print(\"Amount of documents with instance = '': {}\".format(es.search(index=INDEX_NAME, body={\"query\":{\"match\": {\"instance\": \"\"}}}, size=1).get(\"hits\", {}).get(\"total\", {}).get(\"value\", 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
